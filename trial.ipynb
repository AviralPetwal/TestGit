{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import SparkContext, SparkConf\n",
    "sc=SparkContext.getOrCreate()\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Introduction to Spark DaataFrame\") \\\n",
    "    .config(\"spark.some.config.option\", \"some value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchaseSchema =StructType([\n",
    "    StructField(\"Date\",DateType(),True),\n",
    "    StructField(\"Time\",StringType(),True),\n",
    "    StructField(\"City\",StringType(),True),\n",
    "    StructField(\"Item\",StringType(),True),\n",
    "    StructField(\"Total\",FloatType(),True),\n",
    "    StructField(\"Payment\", StringType(),True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchaseDataFrame =spark.read.csv(\"C:\\sparkProjectFolder\\purchase.csv\",header=False,schema=purchaseSchema, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+-----+-------+\n",
      "|Date|Time|City|Item|Total|Payment|\n",
      "+----+----+----+----+-----+-------+\n",
      "|null|null|null|null| null|   null|\n",
      "|null|null|null|null| null|   null|\n",
      "|null|null|null|null| null|   null|\n",
      "|null|null|null|null| null|   null|\n",
      "|null|null|null|null| null|   null|\n",
      "|null|null|null|null| null|   null|\n",
      "|null|null|null|null| null|   null|\n",
      "|null|null|null|null| null|   null|\n",
      "|null|null|null|null| null|   null|\n",
      "|null|null|null|null| null|   null|\n",
      "|null|null|null|null| null|   null|\n",
      "|null|null|null|null| null|   null|\n",
      "|null|null|null|null| null|   null|\n",
      "|null|null|null|null| null|   null|\n",
      "|null|null|null|null| null|   null|\n",
      "|null|null|null|null| null|   null|\n",
      "|null|null|null|null| null|   null|\n",
      "|null|null|null|null| null|   null|\n",
      "|null|null|null|null| null|   null|\n",
      "|null|null|null|null| null|   null|\n",
      "+----+----+----+----+-----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "purchaseDataFrame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Date', 'Time'),\n",
       " ('1/1/2012', '9:00'),\n",
       " ('1/1/2012', '10:00'),\n",
       " ('1/1/2012', '11:00'),\n",
       " ('1/1/2012', '12:00'),\n",
       " ('1/1/2012', '13:00'),\n",
       " ('1/1/2012', '14:00'),\n",
       " ('1/1/2012', '15:00'),\n",
       " ('1/1/2012', '16:00'),\n",
       " ('1/1/2012', '17:00'),\n",
       " ('1/1/2012', '18:00'),\n",
       " ('1/1/2012', '19:00'),\n",
       " ('1/1/2012', '20:00'),\n",
       " ('1/1/2012', '21:00'),\n",
       " ('1/1/2012', '22:00'),\n",
       " ('1/1/2012', '23:00'),\n",
       " ('1/1/2012', '0:00'),\n",
       " ('1/1/2012', '1:00'),\n",
       " ('1/1/2012', '2:00'),\n",
       " ('1/1/2012', '3:00'),\n",
       " ('1/1/2012', '4:00'),\n",
       " ('1/1/2012', '5:00'),\n",
       " ('1/1/2012', '6:00'),\n",
       " ('1/1/2012', '7:00'),\n",
       " ('1/1/2012', '8:00'),\n",
       " ('1/1/2012', '9:00'),\n",
       " ('1/1/2012', '10:00'),\n",
       " ('1/1/2012', '11:00'),\n",
       " ('1/1/2012', '12:00'),\n",
       " ('1/1/2012', '13:00'),\n",
       " ('1/1/2012', '14:00'),\n",
       " ('1/1/2012', '15:00'),\n",
       " ('1/1/2012', '16:00'),\n",
       " ('1/1/2012', '17:00'),\n",
       " ('1/1/2012', '18:00'),\n",
       " ('1/1/2012', '19:00'),\n",
       " ('1/1/2012', '20:00'),\n",
       " ('1/1/2012', '21:00'),\n",
       " ('1/1/2012', '22:00'),\n",
       " ('1/1/2012', '23:00'),\n",
       " ('1/1/2012', '0:00'),\n",
       " ('1/1/2012', '1:00'),\n",
       " ('1/1/2012', '2:00'),\n",
       " ('1/1/2012', '3:00'),\n",
       " ('1/1/2012', '4:00'),\n",
       " ('1/1/2012', '5:00'),\n",
       " ('1/1/2012', '6:00'),\n",
       " ('1/1/2012', '7:00'),\n",
       " ('1/1/2012', '8:00'),\n",
       " ('1/1/2012', '9:00'),\n",
       " ('1/1/2012', '10:00'),\n",
       " ('1/1/2012', '11:00'),\n",
       " ('1/1/2012', '12:00'),\n",
       " ('1/1/2012', '13:00'),\n",
       " ('1/1/2012', '14:00'),\n",
       " ('1/1/2012', '15:00'),\n",
       " ('1/1/2012', '16:00'),\n",
       " ('1/1/2012', '17:00'),\n",
       " ('1/1/2012', '18:00'),\n",
       " ('1/1/2012', '19:00'),\n",
       " ('1/1/2012', '20:00'),\n",
       " ('1/1/2012', '21:00'),\n",
       " ('1/1/2012', '22:00'),\n",
       " ('1/1/2012', '23:00'),\n",
       " ('1/1/2012', '0:00'),\n",
       " ('1/1/2012', '1:00'),\n",
       " ('1/1/2012', '2:00'),\n",
       " ('1/1/2012', '3:00'),\n",
       " ('1/1/2012', '4:00'),\n",
       " ('1/1/2012', '5:00'),\n",
       " ('1/1/2012', '6:00'),\n",
       " ('1/1/2012', '7:00'),\n",
       " ('1/1/2012', '8:00'),\n",
       " ('1/1/2012', '9:00'),\n",
       " ('1/1/2012', '10:00'),\n",
       " ('1/1/2012', '11:00'),\n",
       " ('1/1/2012', '12:00'),\n",
       " ('1/1/2012', '13:00'),\n",
       " ('1/1/2012', '14:00'),\n",
       " ('1/1/2012', '15:00'),\n",
       " ('1/1/2012', '16:00'),\n",
       " ('1/1/2012', '17:00'),\n",
       " ('1/1/2012', '18:00'),\n",
       " ('1/1/2012', '19:00'),\n",
       " ('1/1/2012', '20:00'),\n",
       " ('1/1/2012', '21:00'),\n",
       " ('1/1/2012', '22:00'),\n",
       " ('1/1/2012', '23:00'),\n",
       " ('1/1/2012', '0:00'),\n",
       " ('1/1/2012', '1:00'),\n",
       " ('1/1/2012', '2:00'),\n",
       " ('1/1/2012', '3:00'),\n",
       " ('1/1/2012', '4:00'),\n",
       " ('1/1/2012', '5:00'),\n",
       " ('1/1/2012', '6:00'),\n",
       " ('1/1/2012', '7:00'),\n",
       " ('1/1/2012', '8:00'),\n",
       " ('1/1/2012', '9:00'),\n",
       " ('1/1/2012', '10:00'),\n",
       " ('1/1/2012', '11:00'),\n",
       " ('1/1/2012', '12:00'),\n",
       " ('1/1/2012', '13:00'),\n",
       " ('1/1/2012', '14:00'),\n",
       " ('1/1/2012', '15:00'),\n",
       " ('1/1/2012', '16:00'),\n",
       " ('1/1/2012', '17:00'),\n",
       " ('1/1/2012', '18:00'),\n",
       " ('1/1/2012', '19:00'),\n",
       " ('1/1/2012', '20:00'),\n",
       " ('1/1/2012', '21:00'),\n",
       " ('1/1/2012', '22:00'),\n",
       " ('1/1/2012', '23:00'),\n",
       " ('1/1/2012', '0:00'),\n",
       " ('1/1/2012', '1:00'),\n",
       " ('1/1/2012', '2:00'),\n",
       " ('1/1/2012', '3:00'),\n",
       " ('1/1/2012', '4:00'),\n",
       " ('1/1/2012', '5:00'),\n",
       " ('1/1/2012', '6:00'),\n",
       " ('1/1/2012', '7:00'),\n",
       " ('1/1/2012', '8:00'),\n",
       " ('1/1/2012', '9:00'),\n",
       " ('1/1/2012', '10:00'),\n",
       " ('1/1/2012', '11:00'),\n",
       " ('1/1/2012', '12:00'),\n",
       " ('1/1/2012', '13:00'),\n",
       " ('1/1/2012', '14:00'),\n",
       " ('1/1/2012', '15:00'),\n",
       " ('1/1/2012', '16:00'),\n",
       " ('1/1/2012', '17:00'),\n",
       " ('1/1/2012', '18:00'),\n",
       " ('1/1/2012', '19:00'),\n",
       " ('1/1/2012', '20:00'),\n",
       " ('1/1/2012', '21:00'),\n",
       " ('1/1/2012', '22:00'),\n",
       " ('1/1/2012', '23:00'),\n",
       " ('1/1/2012', '0:00'),\n",
       " ('1/1/2012', '1:00'),\n",
       " ('1/1/2012', '2:00'),\n",
       " ('1/1/2012', '3:00'),\n",
       " ('1/1/2012', '4:00'),\n",
       " ('1/1/2012', '5:00'),\n",
       " ('1/1/2012', '6:00'),\n",
       " ('1/1/2012', '7:00'),\n",
       " ('1/1/2012', '8:00'),\n",
       " ('1/1/2012', '9:00'),\n",
       " ('1/1/2012', '10:00'),\n",
       " ('1/1/2012', '11:00'),\n",
       " ('1/1/2012', '12:00'),\n",
       " ('1/1/2012', '13:00'),\n",
       " ('1/1/2012', '14:00'),\n",
       " ('1/1/2012', '15:00'),\n",
       " ('1/1/2012', '16:00'),\n",
       " ('1/1/2012', '17:00'),\n",
       " ('1/1/2012', '18:00'),\n",
       " ('1/1/2012', '19:00'),\n",
       " ('1/1/2012', '20:00'),\n",
       " ('1/1/2012', '21:00'),\n",
       " ('1/1/2012', '22:00'),\n",
       " ('1/1/2012', '23:00'),\n",
       " ('1/1/2012', '0:00'),\n",
       " ('1/1/2012', '1:00'),\n",
       " ('1/1/2012', '2:00'),\n",
       " ('1/1/2012', '3:00'),\n",
       " ('1/1/2012', '4:00'),\n",
       " ('1/1/2012', '5:00'),\n",
       " ('1/1/2012', '6:00'),\n",
       " ('1/1/2012', '7:00'),\n",
       " ('1/1/2012', '8:00'),\n",
       " ('1/1/2012', '9:00'),\n",
       " ('1/1/2012', '10:00'),\n",
       " ('1/1/2012', '11:00'),\n",
       " ('1/1/2012', '12:00'),\n",
       " ('1/1/2012', '13:00'),\n",
       " ('1/1/2012', '14:00'),\n",
       " ('1/1/2012', '15:00'),\n",
       " ('1/1/2012', '16:00'),\n",
       " ('1/1/2012', '17:00'),\n",
       " ('1/1/2012', '18:00'),\n",
       " ('1/1/2012', '19:00'),\n",
       " ('1/1/2012', '20:00'),\n",
       " ('1/1/2012', '21:00'),\n",
       " ('1/1/2012', '22:00'),\n",
       " ('1/1/2012', '23:00'),\n",
       " ('1/1/2012', '0:00'),\n",
       " ('1/1/2012', '1:00'),\n",
       " ('1/1/2012', '2:00'),\n",
       " ('1/1/2012', '3:00'),\n",
       " ('1/1/2012', '4:00'),\n",
       " ('1/1/2012', '5:00'),\n",
       " ('1/1/2012', '6:00'),\n",
       " ('1/1/2012', '7:00'),\n",
       " ('1/1/2012', '8:00'),\n",
       " ('1/1/2012', '9:00'),\n",
       " ('1/1/2012', '10:00'),\n",
       " ('1/1/2012', '11:00'),\n",
       " ('1/1/2012', '12:00'),\n",
       " ('1/1/2012', '13:00'),\n",
       " ('1/1/2012', '14:00'),\n",
       " ('1/1/2012', '15:00'),\n",
       " ('1/1/2012', '16:00'),\n",
       " ('1/1/2012', '17:00'),\n",
       " ('1/1/2012', '18:00'),\n",
       " ('1/1/2012', '19:00'),\n",
       " ('1/1/2012', '20:00'),\n",
       " ('1/1/2012', '21:00'),\n",
       " ('1/1/2012', '22:00'),\n",
       " ('1/1/2012', '23:00'),\n",
       " ('1/1/2012', '0:00'),\n",
       " ('1/1/2012', '1:00'),\n",
       " ('1/1/2012', '2:00'),\n",
       " ('1/1/2012', '3:00'),\n",
       " ('1/1/2012', '4:00'),\n",
       " ('1/1/2012', '5:00'),\n",
       " ('1/1/2012', '6:00'),\n",
       " ('1/1/2012', '7:00'),\n",
       " ('1/1/2012', '8:00'),\n",
       " ('1/1/2012', '9:00'),\n",
       " ('1/1/2012', '10:00'),\n",
       " ('1/1/2012', '11:00'),\n",
       " ('1/1/2012', '12:00'),\n",
       " ('1/1/2012', '13:00'),\n",
       " ('1/1/2012', '14:00'),\n",
       " ('1/1/2012', '15:00'),\n",
       " ('1/1/2012', '16:00'),\n",
       " ('1/1/2012', '17:00'),\n",
       " ('1/1/2012', '18:00'),\n",
       " ('1/1/2012', '19:00'),\n",
       " ('1/1/2012', '20:00'),\n",
       " ('1/1/2012', '21:00'),\n",
       " ('1/1/2012', '22:00'),\n",
       " ('1/1/2012', '23:00'),\n",
       " ('1/1/2012', '0:00'),\n",
       " ('1/1/2012', '1:00'),\n",
       " ('1/1/2012', '2:00'),\n",
       " ('1/1/2012', '3:00'),\n",
       " ('1/1/2012', '4:00'),\n",
       " ('1/1/2012', '5:00'),\n",
       " ('1/1/2012', '6:00'),\n",
       " ('1/1/2012', '7:00'),\n",
       " ('1/1/2012', '8:00'),\n",
       " ('1/1/2012', '9:00'),\n",
       " ('1/1/2012', '10:00'),\n",
       " ('1/1/2012', '11:00'),\n",
       " ('1/1/2012', '12:00'),\n",
       " ('1/1/2012', '13:00'),\n",
       " ('1/1/2012', '14:00'),\n",
       " ('1/1/2012', '15:00'),\n",
       " ('1/1/2012', '16:00'),\n",
       " ('1/1/2012', '17:00'),\n",
       " ('1/1/2012', '18:00'),\n",
       " ('1/1/2012', '19:00'),\n",
       " ('1/1/2012', '20:00'),\n",
       " ('1/1/2012', '21:00'),\n",
       " ('1/1/2012', '22:00'),\n",
       " ('1/1/2012', '23:00'),\n",
       " ('1/1/2012', '0:00'),\n",
       " ('1/1/2012', '1:00'),\n",
       " ('1/1/2012', '2:00'),\n",
       " ('1/1/2012', '3:00'),\n",
       " ('1/1/2012', '4:00'),\n",
       " ('1/1/2012', '5:00'),\n",
       " ('1/1/2012', '6:00'),\n",
       " ('1/1/2012', '7:00'),\n",
       " ('1/1/2012', '8:00'),\n",
       " ('1/1/2012', '9:00'),\n",
       " ('1/1/2012', '10:00'),\n",
       " ('1/1/2012', '11:00'),\n",
       " ('1/1/2012', '12:00'),\n",
       " ('1/1/2012', '13:00'),\n",
       " ('1/1/2012', '14:00'),\n",
       " ('1/1/2012', '15:00'),\n",
       " ('1/1/2012', '16:00'),\n",
       " ('1/1/2012', '17:00'),\n",
       " ('1/1/2012', '18:00'),\n",
       " ('1/1/2012', '19:00'),\n",
       " ('1/1/2012', '20:00'),\n",
       " ('1/1/2012', '21:00'),\n",
       " ('1/1/2012', '22:00'),\n",
       " ('1/1/2012', '23:00'),\n",
       " ('1/1/2012', '0:00'),\n",
       " ('1/1/2012', '1:00'),\n",
       " ('1/1/2012', '2:00'),\n",
       " ('1/1/2012', '3:00'),\n",
       " ('1/1/2012', '4:00'),\n",
       " ('1/1/2012', '5:00'),\n",
       " ('1/1/2012', '6:00'),\n",
       " ('1/1/2012', '7:00'),\n",
       " ('1/1/2012', '8:00'),\n",
       " ('1/1/2012', '9:00'),\n",
       " ('1/1/2012', '10:00'),\n",
       " ('1/1/2012', '11:00'),\n",
       " ('1/1/2012', '12:00'),\n",
       " ('1/1/2012', '13:00'),\n",
       " ('1/1/2012', '14:00'),\n",
       " ('1/1/2012', '15:00'),\n",
       " ('1/1/2012', '16:00'),\n",
       " ('1/1/2012', '17:00'),\n",
       " ('1/1/2012', '18:00'),\n",
       " ('1/1/2012', '19:00'),\n",
       " ('1/1/2012', '20:00'),\n",
       " ('1/1/2012', '21:00'),\n",
       " ('1/1/2012', '22:00'),\n",
       " ('1/1/2012', '23:00'),\n",
       " ('1/1/2012', '0:00'),\n",
       " ('1/1/2012', '1:00'),\n",
       " ('1/1/2012', '2:00'),\n",
       " ('1/1/2012', '3:00'),\n",
       " ('1/1/2012', '4:00'),\n",
       " ('1/1/2012', '5:00'),\n",
       " ('1/1/2012', '6:00'),\n",
       " ('1/1/2012', '7:00'),\n",
       " ('1/1/2012', '8:00'),\n",
       " ('1/1/2012', '9:00'),\n",
       " ('1/1/2012', '10:00'),\n",
       " ('1/1/2012', '11:00'),\n",
       " ('1/1/2012', '12:00'),\n",
       " ('1/1/2012', '13:00'),\n",
       " ('1/1/2012', '14:00'),\n",
       " ('1/1/2012', '15:00'),\n",
       " ('1/1/2012', '16:00'),\n",
       " ('1/1/2012', '17:00'),\n",
       " ('1/1/2012', '18:00'),\n",
       " ('1/1/2012', '19:00'),\n",
       " ('1/1/2012', '20:00'),\n",
       " ('1/1/2012', '21:00'),\n",
       " ('1/1/2012', '22:00'),\n",
       " ('1/1/2012', '23:00'),\n",
       " ('1/1/2012', '0:00'),\n",
       " ('1/1/2012', '1:00'),\n",
       " ('1/1/2012', '2:00'),\n",
       " ('1/1/2012', '3:00'),\n",
       " ('1/1/2012', '4:00'),\n",
       " ('1/1/2012', '5:00'),\n",
       " ('1/1/2012', '6:00'),\n",
       " ('1/1/2012', '7:00'),\n",
       " ('1/1/2012', '8:00'),\n",
       " ('1/1/2012', '9:00'),\n",
       " ('1/1/2012', '10:00'),\n",
       " ('1/1/2012', '11:00'),\n",
       " ('1/1/2012', '12:00'),\n",
       " ('1/1/2012', '13:00'),\n",
       " ('1/1/2012', '14:00'),\n",
       " ('1/1/2012', '15:00'),\n",
       " ('1/1/2012', '16:00'),\n",
       " ('1/1/2012', '17:00'),\n",
       " ('1/1/2012', '18:00'),\n",
       " ('1/1/2012', '19:00'),\n",
       " ('1/1/2012', '20:00'),\n",
       " ('1/1/2012', '21:00'),\n",
       " ('1/1/2012', '22:00'),\n",
       " ('1/1/2012', '23:00'),\n",
       " ('1/1/2012', '0:00'),\n",
       " ('1/1/2012', '1:00'),\n",
       " ('1/1/2012', '2:00'),\n",
       " ('1/1/2012', '3:00'),\n",
       " ('1/1/2012', '4:00'),\n",
       " ('1/1/2012', '5:00'),\n",
       " ('1/1/2012', '6:00'),\n",
       " ('1/1/2012', '7:00'),\n",
       " ('1/1/2012', '8:00'),\n",
       " ('1/1/2012', '9:00'),\n",
       " ('1/1/2012', '10:00'),\n",
       " ('1/1/2012', '11:00'),\n",
       " ('1/1/2012', '12:00'),\n",
       " ('1/1/2012', '13:00'),\n",
       " ('1/1/2012', '14:00'),\n",
       " ('1/1/2012', '15:00'),\n",
       " ('1/1/2012', '16:00'),\n",
       " ('1/1/2012', '17:00'),\n",
       " ('1/1/2012', '18:00'),\n",
       " ('1/1/2012', '19:00'),\n",
       " ('1/1/2012', '20:00'),\n",
       " ('1/1/2012', '21:00'),\n",
       " ('1/1/2012', '22:00'),\n",
       " ('1/1/2012', '23:00'),\n",
       " ('1/1/2012', '0:00'),\n",
       " ('1/1/2012', '1:00'),\n",
       " ('1/1/2012', '2:00'),\n",
       " ('1/1/2012', '3:00'),\n",
       " ('1/1/2012', '4:00'),\n",
       " ('1/1/2012', '5:00'),\n",
       " ('1/1/2012', '6:00'),\n",
       " ('1/1/2012', '7:00'),\n",
       " ('1/1/2012', '8:00'),\n",
       " ('1/1/2012', '9:00'),\n",
       " ('1/1/2012', '10:00'),\n",
       " ('1/1/2012', '11:00'),\n",
       " ('1/1/2012', '12:00'),\n",
       " ('1/1/2012', '13:00'),\n",
       " ('1/1/2012', '14:00'),\n",
       " ('1/1/2012', '15:00'),\n",
       " ('1/1/2012', '16:00'),\n",
       " ('1/1/2012', '17:00'),\n",
       " ('1/1/2012', '18:00'),\n",
       " ('1/1/2012', '19:00'),\n",
       " ('1/1/2012', '20:00'),\n",
       " ('1/1/2012', '21:00'),\n",
       " ('1/1/2012', '22:00'),\n",
       " ('1/1/2012', '23:00'),\n",
       " ('1/1/2012', '0:00'),\n",
       " ('1/1/2012', '1:00'),\n",
       " ('1/1/2012', '2:00'),\n",
       " ('1/1/2012', '3:00'),\n",
       " ('1/1/2012', '4:00'),\n",
       " ('1/1/2012', '5:00'),\n",
       " ('1/1/2012', '6:00'),\n",
       " ('1/1/2012', '7:00'),\n",
       " ('1/1/2012', '8:00'),\n",
       " ('1/1/2012', '9:00'),\n",
       " ('1/1/2012', '10:00'),\n",
       " ('1/1/2012', '11:00'),\n",
       " ('1/1/2012', '12:00'),\n",
       " ('1/1/2012', '13:00'),\n",
       " ('1/1/2012', '14:00'),\n",
       " ('1/1/2012', '15:00'),\n",
       " ('1/1/2012', '16:00'),\n",
       " ('1/1/2012', '17:00'),\n",
       " ('1/1/2012', '18:00'),\n",
       " ('1/1/2012', '19:00'),\n",
       " ('1/1/2012', '20:00'),\n",
       " ('1/1/2012', '21:00'),\n",
       " ('1/1/2012', '22:00'),\n",
       " ('1/1/2012', '23:00'),\n",
       " ('1/1/2012', '0:00'),\n",
       " ('1/1/2012', '1:00'),\n",
       " ('1/1/2012', '2:00'),\n",
       " ('1/1/2012', '3:00'),\n",
       " ('1/1/2012', '4:00'),\n",
       " ('1/1/2012', '5:00'),\n",
       " ('1/1/2012', '6:00'),\n",
       " ('1/1/2012', '7:00'),\n",
       " ('1/1/2012', '8:00'),\n",
       " ('1/1/2012', '9:00'),\n",
       " ('1/1/2012', '10:00'),\n",
       " ('1/1/2012', '11:00'),\n",
       " ('1/1/2012', '12:00'),\n",
       " ('1/1/2012', '13:00'),\n",
       " ('1/1/2012', '14:00'),\n",
       " ('1/1/2012', '15:00'),\n",
       " ('1/1/2012', '16:00'),\n",
       " ('1/1/2012', '17:00'),\n",
       " ('1/1/2012', '18:00'),\n",
       " ('1/1/2012', '19:00'),\n",
       " ('1/1/2012', '20:00'),\n",
       " ('1/1/2012', '21:00'),\n",
       " ('1/1/2012', '22:00'),\n",
       " ('1/1/2012', '23:00'),\n",
       " ('1/1/2012', '0:00'),\n",
       " ('1/1/2012', '1:00'),\n",
       " ('1/1/2012', '2:00'),\n",
       " ('1/1/2012', '3:00'),\n",
       " ('1/1/2012', '4:00'),\n",
       " ('1/1/2012', '5:00'),\n",
       " ('1/1/2012', '6:00'),\n",
       " ('1/1/2012', '7:00'),\n",
       " ('1/1/2012', '8:00'),\n",
       " ('1/1/2012', '9:00'),\n",
       " ('1/1/2012', '10:00'),\n",
       " ('1/1/2012', '11:00'),\n",
       " ('1/1/2012', '12:00'),\n",
       " ('1/1/2012', '13:00'),\n",
       " ('1/1/2012', '14:00'),\n",
       " ('1/1/2012', '15:00'),\n",
       " ('1/1/2012', '16:00'),\n",
       " ('1/1/2012', '17:00'),\n",
       " ('1/1/2012', '18:00'),\n",
       " ('1/1/2012', '19:00'),\n",
       " ('1/1/2012', '20:00'),\n",
       " ('1/1/2012', '21:00'),\n",
       " ('1/1/2012', '22:00'),\n",
       " ('1/1/2012', '23:00'),\n",
       " ('1/1/2012', '0:00'),\n",
       " ('1/1/2012', '1:00'),\n",
       " ('1/1/2012', '2:00'),\n",
       " ('1/1/2012', '3:00'),\n",
       " ('1/1/2012', '4:00'),\n",
       " ('1/1/2012', '5:00'),\n",
       " ('1/1/2012', '6:00'),\n",
       " ('1/1/2012', '7:00'),\n",
       " ('1/1/2012', '8:00'),\n",
       " ('1/1/2012', '9:00'),\n",
       " ('1/1/2012', '10:00'),\n",
       " ('1/1/2012', '11:00'),\n",
       " ('1/1/2012', '12:00'),\n",
       " ('1/1/2012', '13:00'),\n",
       " ('1/1/2012', '14:00'),\n",
       " ('1/1/2012', '15:00'),\n",
       " ('1/1/2012', '16:00'),\n",
       " ('1/1/2012', '17:00'),\n",
       " ('1/1/2012', '18:00'),\n",
       " ('1/1/2012', '19:00'),\n",
       " ('1/1/2012', '20:00'),\n",
       " ('1/1/2012', '21:00'),\n",
       " ('1/1/2012', '22:00'),\n",
       " ('1/1/2012', '23:00'),\n",
       " ('1/1/2012', '0:00'),\n",
       " ('1/1/2012', '1:00'),\n",
       " ('1/1/2012', '2:00'),\n",
       " ('1/1/2012', '3:00'),\n",
       " ('1/1/2012', '4:00'),\n",
       " ('1/1/2012', '5:00'),\n",
       " ('1/1/2012', '6:00'),\n",
       " ('1/1/2012', '7:00'),\n",
       " ('1/1/2012', '8:00'),\n",
       " ('1/1/2012', '9:00'),\n",
       " ('1/1/2012', '10:00'),\n",
       " ('1/1/2012', '11:00'),\n",
       " ('1/1/2012', '12:00'),\n",
       " ('1/1/2012', '13:00'),\n",
       " ('1/1/2012', '14:00'),\n",
       " ('1/1/2012', '15:00'),\n",
       " ('1/1/2012', '16:00'),\n",
       " ('1/1/2012', '17:00'),\n",
       " ('1/1/2012', '18:00'),\n",
       " ('1/1/2012', '19:00'),\n",
       " ('1/1/2012', '20:00'),\n",
       " ('1/1/2012', '21:00'),\n",
       " ('1/1/2012', '22:00'),\n",
       " ('1/1/2012', '23:00'),\n",
       " ('1/1/2012', '0:00'),\n",
       " ('1/1/2012', '1:00'),\n",
       " ('1/1/2012', '2:00'),\n",
       " ('1/1/2012', '3:00'),\n",
       " ('1/1/2012', '4:00'),\n",
       " ('1/1/2012', '5:00'),\n",
       " ('1/1/2012', '6:00'),\n",
       " ('1/1/2012', '7:00'),\n",
       " ('1/1/2012', '8:00'),\n",
       " ('1/1/2012', '9:00'),\n",
       " ('1/1/2012', '10:00'),\n",
       " ('1/1/2012', '11:00'),\n",
       " ('1/1/2012', '12:00'),\n",
       " ('1/1/2012', '13:00'),\n",
       " ('1/1/2012', '14:00'),\n",
       " ('1/1/2012', '15:00'),\n",
       " ('1/1/2012', '16:00'),\n",
       " ('1/1/2012', '17:00'),\n",
       " ('1/1/2012', '18:00'),\n",
       " ('1/1/2012', '19:00'),\n",
       " ('1/1/2012', '20:00'),\n",
       " ('1/1/2012', '21:00'),\n",
       " ('1/1/2012', '22:00'),\n",
       " ('1/1/2012', '23:00'),\n",
       " ('1/1/2012', '0:00'),\n",
       " ('1/1/2012', '1:00'),\n",
       " ('1/1/2012', '2:00'),\n",
       " ('1/1/2012', '3:00'),\n",
       " ('1/1/2012', '4:00'),\n",
       " ('1/1/2012', '5:00'),\n",
       " ('1/1/2012', '6:00'),\n",
       " ('1/1/2012', '7:00'),\n",
       " ('1/1/2012', '8:00'),\n",
       " ('1/1/2012', '9:00'),\n",
       " ('1/1/2012', '10:00'),\n",
       " ('1/1/2012', '11:00'),\n",
       " ('1/1/2012', '12:00'),\n",
       " ('1/1/2012', '13:00'),\n",
       " ('1/1/2012', '14:00'),\n",
       " ('1/1/2012', '15:00'),\n",
       " ('1/1/2012', '16:00'),\n",
       " ('1/1/2012', '17:00'),\n",
       " ('1/1/2012', '18:00'),\n",
       " ('1/1/2012', '19:00'),\n",
       " ('1/1/2012', '20:00'),\n",
       " ('1/1/2012', '21:00'),\n",
       " ('1/1/2012', '22:00'),\n",
       " ('1/1/2012', '23:00'),\n",
       " ('1/1/2012', '0:00'),\n",
       " ('1/1/2012', '1:00'),\n",
       " ('1/1/2012', '2:00'),\n",
       " ('1/1/2012', '3:00'),\n",
       " ('1/1/2012', '4:00'),\n",
       " ('1/1/2012', '5:00'),\n",
       " ('1/1/2012', '6:00'),\n",
       " ('1/1/2012', '7:00'),\n",
       " ('1/1/2012', '8:00'),\n",
       " ('1/1/2012', '9:00'),\n",
       " ('1/1/2012', '10:00'),\n",
       " ('1/1/2012', '11:00'),\n",
       " ('1/1/2012', '12:00'),\n",
       " ('1/1/2012', '13:00'),\n",
       " ('1/1/2012', '14:00'),\n",
       " ('1/1/2012', '15:00'),\n",
       " ('1/1/2012', '16:00'),\n",
       " ('1/1/2012', '17:00'),\n",
       " ('1/1/2012', '18:00'),\n",
       " ('1/1/2012', '19:00'),\n",
       " ('1/1/2012', '20:00'),\n",
       " ('1/1/2012', '21:00'),\n",
       " ('1/1/2012', '22:00'),\n",
       " ('1/1/2012', '23:00'),\n",
       " ('1/1/2012', '0:00'),\n",
       " ('1/1/2012', '1:00'),\n",
       " ('1/1/2012', '2:00'),\n",
       " ('1/1/2012', '3:00'),\n",
       " ('1/1/2012', '4:00'),\n",
       " ('1/1/2012', '5:00'),\n",
       " ('1/1/2012', '6:00'),\n",
       " ('1/1/2012', '7:00'),\n",
       " ('1/1/2012', '8:00'),\n",
       " ('1/1/2012', '9:00'),\n",
       " ('1/1/2012', '10:00'),\n",
       " ('1/1/2012', '11:00'),\n",
       " ('1/1/2012', '12:00'),\n",
       " ('1/1/2012', '13:00'),\n",
       " ('1/1/2012', '14:00'),\n",
       " ('1/1/2012', '15:00'),\n",
       " ('1/1/2012', '16:00'),\n",
       " ('1/1/2012', '17:00'),\n",
       " ('1/1/2012', '18:00'),\n",
       " ('1/1/2012', '19:00'),\n",
       " ('1/1/2012', '20:00'),\n",
       " ('1/1/2012', '21:00'),\n",
       " ('1/1/2012', '22:00'),\n",
       " ('1/1/2012', '23:00'),\n",
       " ('1/1/2012', '0:00'),\n",
       " ('1/1/2012', '1:00'),\n",
       " ('1/1/2012', '2:00'),\n",
       " ('1/1/2012', '3:00'),\n",
       " ('1/1/2012', '4:00'),\n",
       " ('1/1/2012', '5:00'),\n",
       " ('1/1/2012', '6:00'),\n",
       " ('1/1/2012', '7:00'),\n",
       " ('1/1/2012', '8:00'),\n",
       " ('1/1/2012', '9:00'),\n",
       " ('1/1/2012', '10:00'),\n",
       " ('1/1/2012', '11:00'),\n",
       " ('1/1/2012', '12:00'),\n",
       " ('1/1/2012', '13:00'),\n",
       " ('1/1/2012', '14:00'),\n",
       " ('1/1/2012', '15:00'),\n",
       " ('1/1/2012', '16:00'),\n",
       " ('1/1/2012', '17:00'),\n",
       " ('1/1/2012', '18:00'),\n",
       " ('1/1/2012', '19:00'),\n",
       " ('1/1/2012', '20:00'),\n",
       " ('1/1/2012', '21:00'),\n",
       " ('1/1/2012', '22:00'),\n",
       " ('1/1/2012', '23:00'),\n",
       " ('1/1/2012', '0:00'),\n",
       " ('1/1/2012', '1:00'),\n",
       " ('1/1/2012', '2:00'),\n",
       " ('1/1/2012', '3:00'),\n",
       " ('1/1/2012', '4:00'),\n",
       " ('1/1/2012', '5:00'),\n",
       " ('1/1/2012', '6:00'),\n",
       " ('1/1/2012', '7:00'),\n",
       " ('1/1/2012', '8:00'),\n",
       " ('1/1/2012', '9:00'),\n",
       " ('1/1/2012', '10:00'),\n",
       " ('1/1/2012', '11:00'),\n",
       " ('1/1/2012', '12:00'),\n",
       " ('1/1/2012', '13:00'),\n",
       " ('1/1/2012', '14:00'),\n",
       " ('1/1/2012', '15:00'),\n",
       " ('1/1/2012', '16:00'),\n",
       " ('1/1/2012', '17:00'),\n",
       " ('1/1/2012', '18:00'),\n",
       " ('1/1/2012', '19:00'),\n",
       " ('1/1/2012', '20:00'),\n",
       " ('1/1/2012', '21:00'),\n",
       " ('1/1/2012', '22:00'),\n",
       " ('1/1/2012', '23:00'),\n",
       " ('1/1/2012', '0:00'),\n",
       " ('1/1/2012', '1:00'),\n",
       " ('1/1/2012', '2:00'),\n",
       " ('1/1/2012', '3:00'),\n",
       " ('1/1/2012', '4:00'),\n",
       " ('1/1/2012', '5:00'),\n",
       " ('1/1/2012', '6:00'),\n",
       " ('1/1/2012', '7:00'),\n",
       " ('1/1/2012', '8:00'),\n",
       " ('1/1/2012', '9:00'),\n",
       " ('1/1/2012', '10:00'),\n",
       " ('1/1/2012', '11:00'),\n",
       " ('1/1/2012', '12:00'),\n",
       " ('1/1/2012', '13:00'),\n",
       " ('1/1/2012', '14:00'),\n",
       " ('1/1/2012', '15:00'),\n",
       " ('1/1/2012', '16:00'),\n",
       " ('1/1/2012', '17:00'),\n",
       " ('1/1/2012', '18:00'),\n",
       " ('1/1/2012', '19:00'),\n",
       " ('1/1/2012', '20:00'),\n",
       " ('1/1/2012', '21:00'),\n",
       " ('1/1/2012', '22:00'),\n",
       " ('1/1/2012', '23:00'),\n",
       " ('1/1/2012', '0:00'),\n",
       " ('1/1/2012', '1:00'),\n",
       " ('1/1/2012', '2:00'),\n",
       " ('1/1/2012', '3:00'),\n",
       " ('1/1/2012', '4:00'),\n",
       " ('1/1/2012', '5:00'),\n",
       " ('1/1/2012', '6:00'),\n",
       " ('1/1/2012', '7:00'),\n",
       " ('1/1/2012', '8:00'),\n",
       " ('1/1/2012', '9:00'),\n",
       " ('1/1/2012', '10:00'),\n",
       " ('1/1/2012', '11:00'),\n",
       " ('1/1/2012', '12:00'),\n",
       " ('1/1/2012', '13:00'),\n",
       " ('1/1/2012', '14:00'),\n",
       " ('1/1/2012', '15:00'),\n",
       " ('1/1/2012', '16:00'),\n",
       " ('1/1/2012', '17:00'),\n",
       " ('1/1/2012', '18:00'),\n",
       " ('1/1/2012', '19:00'),\n",
       " ('1/1/2012', '20:00'),\n",
       " ('1/1/2012', '21:00'),\n",
       " ('1/1/2012', '22:00'),\n",
       " ('1/1/2012', '23:00'),\n",
       " ('1/1/2012', '0:00'),\n",
       " ('1/1/2012', '1:00'),\n",
       " ('1/1/2012', '2:00'),\n",
       " ('1/1/2012', '3:00'),\n",
       " ('1/1/2012', '4:00'),\n",
       " ('1/1/2012', '5:00'),\n",
       " ('1/1/2012', '6:00'),\n",
       " ('1/1/2012', '7:00'),\n",
       " ('1/1/2012', '8:00'),\n",
       " ('1/1/2012', '9:00'),\n",
       " ('1/1/2012', '10:00'),\n",
       " ('1/1/2012', '11:00'),\n",
       " ('1/1/2012', '12:00'),\n",
       " ('1/1/2012', '13:00'),\n",
       " ('1/1/2012', '14:00'),\n",
       " ('1/1/2012', '15:00'),\n",
       " ('1/1/2012', '16:00'),\n",
       " ('1/1/2012', '17:00'),\n",
       " ('1/1/2012', '18:00'),\n",
       " ('1/1/2012', '19:00'),\n",
       " ('1/1/2012', '20:00'),\n",
       " ('1/1/2012', '21:00'),\n",
       " ('1/1/2012', '22:00'),\n",
       " ('1/1/2012', '23:00'),\n",
       " ('1/1/2012', '0:00'),\n",
       " ('1/1/2012', '1:00'),\n",
       " ('1/1/2012', '2:00'),\n",
       " ('1/1/2012', '3:00'),\n",
       " ('1/1/2012', '4:00'),\n",
       " ('1/1/2012', '5:00'),\n",
       " ('1/1/2012', '6:00'),\n",
       " ('1/1/2012', '7:00'),\n",
       " ('1/1/2012', '8:00'),\n",
       " ('1/1/2012', '9:00'),\n",
       " ('1/1/2012', '10:00'),\n",
       " ('1/1/2012', '11:00'),\n",
       " ('1/1/2012', '12:00'),\n",
       " ('1/1/2012', '13:00'),\n",
       " ('1/1/2012', '14:00'),\n",
       " ('1/1/2012', '15:00'),\n",
       " ('1/1/2012', '16:00'),\n",
       " ('1/1/2012', '17:00'),\n",
       " ('1/1/2012', '18:00'),\n",
       " ('1/1/2012', '19:00'),\n",
       " ('1/1/2012', '20:00'),\n",
       " ('1/1/2012', '21:00'),\n",
       " ('1/1/2012', '22:00'),\n",
       " ('1/1/2012', '23:00'),\n",
       " ('1/1/2012', '0:00'),\n",
       " ('1/1/2012', '1:00'),\n",
       " ('1/1/2012', '2:00'),\n",
       " ('1/1/2012', '3:00'),\n",
       " ('1/1/2012', '4:00'),\n",
       " ('1/1/2012', '5:00'),\n",
       " ('1/1/2012', '6:00'),\n",
       " ('1/1/2012', '7:00'),\n",
       " ('1/1/2012', '8:00'),\n",
       " ('1/1/2012', '9:00'),\n",
       " ('1/1/2012', '10:00'),\n",
       " ('1/1/2012', '11:00'),\n",
       " ('1/1/2012', '12:00'),\n",
       " ('1/1/2012', '13:00'),\n",
       " ('1/1/2012', '14:00'),\n",
       " ('1/1/2012', '15:00'),\n",
       " ('1/1/2012', '16:00'),\n",
       " ('1/1/2012', '17:00'),\n",
       " ('1/1/2012', '18:00'),\n",
       " ('1/1/2012', '19:00'),\n",
       " ('1/1/2012', '20:00'),\n",
       " ('1/1/2012', '21:00'),\n",
       " ('1/1/2012', '22:00'),\n",
       " ('1/1/2012', '23:00'),\n",
       " ('1/1/2012', '0:00'),\n",
       " ('1/1/2012', '1:00'),\n",
       " ('1/1/2012', '2:00'),\n",
       " ('1/1/2012', '3:00'),\n",
       " ('1/1/2012', '4:00'),\n",
       " ('1/1/2012', '5:00'),\n",
       " ('1/1/2012', '6:00'),\n",
       " ('1/1/2012', '7:00'),\n",
       " ('1/1/2012', '8:00'),\n",
       " ('1/1/2012', '9:00'),\n",
       " ('1/1/2012', '10:00'),\n",
       " ('1/1/2012', '11:00'),\n",
       " ('1/1/2012', '12:00'),\n",
       " ('1/1/2012', '13:00'),\n",
       " ('1/1/2012', '14:00'),\n",
       " ('1/1/2012', '15:00'),\n",
       " ('1/1/2012', '16:00'),\n",
       " ('1/1/2012', '17:00'),\n",
       " ('1/1/2012', '18:00'),\n",
       " ('1/1/2012', '19:00'),\n",
       " ('1/1/2012', '20:00'),\n",
       " ('1/1/2012', '21:00'),\n",
       " ('1/1/2012', '22:00'),\n",
       " ('1/1/2012', '23:00'),\n",
       " ('1/1/2012', '0:00'),\n",
       " ('1/1/2012', '1:00'),\n",
       " ('1/1/2012', '2:00'),\n",
       " ('1/1/2012', '3:00'),\n",
       " ('1/1/2012', '4:00'),\n",
       " ('1/1/2012', '5:00'),\n",
       " ('1/1/2012', '6:00'),\n",
       " ('1/1/2012', '7:00'),\n",
       " ('1/1/2012', '8:00'),\n",
       " ('1/1/2012', '9:00'),\n",
       " ('1/1/2012', '10:00'),\n",
       " ('1/1/2012', '11:00'),\n",
       " ('1/1/2012', '12:00'),\n",
       " ('1/1/2012', '13:00'),\n",
       " ('1/1/2012', '14:00'),\n",
       " ('1/1/2012', '15:00'),\n",
       " ('1/1/2012', '16:00'),\n",
       " ('1/1/2012', '17:00'),\n",
       " ('1/1/2012', '18:00'),\n",
       " ('1/1/2012', '19:00'),\n",
       " ('1/1/2012', '20:00'),\n",
       " ('1/1/2012', '21:00'),\n",
       " ('1/1/2012', '22:00'),\n",
       " ('1/1/2012', '23:00'),\n",
       " ('1/1/2012', '0:00'),\n",
       " ('1/1/2012', '1:00'),\n",
       " ('1/1/2012', '2:00'),\n",
       " ('1/1/2012', '3:00'),\n",
       " ('1/1/2012', '4:00'),\n",
       " ('1/1/2012', '5:00'),\n",
       " ('1/1/2012', '6:00'),\n",
       " ('1/1/2012', '7:00'),\n",
       " ('1/1/2012', '8:00'),\n",
       " ('1/1/2012', '9:00'),\n",
       " ('1/1/2012', '10:00'),\n",
       " ('1/1/2012', '11:00'),\n",
       " ('1/1/2012', '12:00'),\n",
       " ('1/1/2012', '13:00'),\n",
       " ('1/1/2012', '14:00'),\n",
       " ('1/1/2012', '15:00'),\n",
       " ('1/1/2012', '16:00'),\n",
       " ('1/1/2012', '17:00'),\n",
       " ('1/1/2012', '18:00'),\n",
       " ('1/1/2012', '19:00'),\n",
       " ('1/1/2012', '20:00'),\n",
       " ('1/1/2012', '21:00'),\n",
       " ('1/1/2012', '22:00'),\n",
       " ('1/1/2012', '23:00'),\n",
       " ('1/1/2012', '0:00'),\n",
       " ('1/1/2012', '1:00'),\n",
       " ('1/1/2012', '2:00'),\n",
       " ('1/1/2012', '3:00'),\n",
       " ('1/1/2012', '4:00'),\n",
       " ('1/1/2012', '5:00'),\n",
       " ('1/1/2012', '6:00'),\n",
       " ('1/1/2012', '7:00'),\n",
       " ('1/1/2012', '8:00'),\n",
       " ('1/1/2012', '9:00'),\n",
       " ('1/1/2012', '10:00'),\n",
       " ('1/1/2012', '11:00'),\n",
       " ('1/1/2012', '12:00'),\n",
       " ('1/1/2012', '13:00'),\n",
       " ('1/1/2012', '14:00'),\n",
       " ('1/1/2012', '15:00'),\n",
       " ('1/1/2012', '16:00'),\n",
       " ('1/1/2012', '17:00'),\n",
       " ('1/1/2012', '18:00'),\n",
       " ('1/1/2012', '19:00'),\n",
       " ('1/1/2012', '20:00'),\n",
       " ('1/1/2012', '21:00'),\n",
       " ('1/1/2012', '22:00'),\n",
       " ('1/1/2012', '23:00'),\n",
       " ('1/1/2012', '0:00'),\n",
       " ('1/1/2012', '1:00'),\n",
       " ('1/1/2012', '2:00'),\n",
       " ('1/1/2012', '3:00'),\n",
       " ('1/1/2012', '4:00'),\n",
       " ('1/1/2012', '5:00'),\n",
       " ('1/1/2012', '6:00'),\n",
       " ('1/1/2012', '7:00'),\n",
       " ('1/1/2012', '8:00'),\n",
       " ('1/1/2012', '9:00'),\n",
       " ('1/1/2012', '10:00'),\n",
       " ('1/1/2012', '11:00'),\n",
       " ('1/1/2012', '12:00'),\n",
       " ('1/1/2012', '13:00'),\n",
       " ('1/1/2012', '14:00'),\n",
       " ('1/1/2012', '15:00'),\n",
       " ('1/1/2012', '16:00'),\n",
       " ('1/1/2012', '17:00'),\n",
       " ('1/1/2012', '18:00'),\n",
       " ('1/1/2012', '19:00'),\n",
       " ('1/1/2012', '20:00'),\n",
       " ('1/1/2012', '21:00'),\n",
       " ('1/1/2012', '22:00'),\n",
       " ('1/1/2012', '23:00'),\n",
       " ('1/1/2012', '0:00'),\n",
       " ('1/1/2012', '1:00'),\n",
       " ('1/1/2012', '2:00'),\n",
       " ('1/1/2012', '3:00'),\n",
       " ('1/1/2012', '4:00'),\n",
       " ('1/1/2012', '5:00'),\n",
       " ('1/1/2012', '6:00'),\n",
       " ('1/1/2012', '7:00'),\n",
       " ('1/1/2012', '8:00'),\n",
       " ('1/1/2012', '9:00'),\n",
       " ('1/1/2012', '10:00'),\n",
       " ('1/1/2012', '11:00'),\n",
       " ('1/1/2012', '12:00'),\n",
       " ('1/1/2012', '13:00'),\n",
       " ('1/1/2012', '14:00'),\n",
       " ('1/1/2012', '15:00'),\n",
       " ('1/1/2012', '16:00'),\n",
       " ('1/1/2012', '17:00'),\n",
       " ('1/1/2012', '18:00'),\n",
       " ('1/1/2012', '19:00'),\n",
       " ('1/1/2012', '20:00'),\n",
       " ('1/1/2012', '21:00'),\n",
       " ('1/1/2012', '22:00'),\n",
       " ('1/1/2012', '23:00'),\n",
       " ('1/1/2012', '0:00'),\n",
       " ('1/1/2012', '1:00'),\n",
       " ('1/1/2012', '2:00'),\n",
       " ('1/1/2012', '3:00'),\n",
       " ('1/1/2012', '4:00'),\n",
       " ('1/1/2012', '5:00'),\n",
       " ('1/1/2012', '6:00'),\n",
       " ('1/1/2012', '7:00'),\n",
       " ('1/1/2012', '8:00'),\n",
       " ('1/1/2012', '9:00'),\n",
       " ('1/1/2012', '10:00'),\n",
       " ('1/1/2012', '11:00'),\n",
       " ('1/1/2012', '12:00'),\n",
       " ('1/1/2012', '13:00'),\n",
       " ('1/1/2012', '14:00'),\n",
       " ('1/1/2012', '15:00'),\n",
       " ('1/1/2012', '16:00'),\n",
       " ('1/1/2012', '17:00'),\n",
       " ('1/1/2012', '18:00'),\n",
       " ('1/1/2012', '19:00'),\n",
       " ('1/1/2012', '20:00'),\n",
       " ('1/1/2012', '21:00'),\n",
       " ('1/1/2012', '22:00'),\n",
       " ('1/1/2012', '23:00'),\n",
       " ('1/1/2012', '0:00'),\n",
       " ('1/1/2012', '1:00'),\n",
       " ('1/1/2012', '2:00'),\n",
       " ('1/1/2012', '3:00'),\n",
       " ('1/1/2012', '4:00'),\n",
       " ('1/1/2012', '5:00'),\n",
       " ('1/1/2012', '6:00'),\n",
       " ('1/1/2012', '7:00'),\n",
       " ('1/1/2012', '8:00'),\n",
       " ('1/1/2012', '9:00'),\n",
       " ('1/1/2012', '10:00'),\n",
       " ('1/1/2012', '11:00'),\n",
       " ('1/1/2012', '12:00'),\n",
       " ('1/1/2012', '13:00'),\n",
       " ('1/1/2012', '14:00'),\n",
       " ('1/1/2012', '15:00'),\n",
       " ('1/1/2012', '16:00'),\n",
       " ('1/1/2012', '17:00'),\n",
       " ('1/1/2012', '18:00'),\n",
       " ('1/1/2012', '19:00'),\n",
       " ('1/1/2012', '20:00'),\n",
       " ('1/1/2012', '21:00'),\n",
       " ('1/1/2012', '22:00'),\n",
       " ('1/1/2012', '23:00'),\n",
       " ('1/1/2012', '0:00'),\n",
       " ('1/1/2012', '1:00'),\n",
       " ('1/1/2012', '2:00'),\n",
       " ('1/1/2012', '3:00'),\n",
       " ('1/1/2012', '4:00'),\n",
       " ('1/1/2012', '5:00'),\n",
       " ('1/1/2012', '6:00'),\n",
       " ('1/1/2012', '7:00'),\n",
       " ('1/1/2012', '8:00'),\n",
       " ('1/1/2012', '9:00'),\n",
       " ('1/1/2012', '10:00'),\n",
       " ('1/1/2012', '11:00'),\n",
       " ('1/1/2012', '12:00'),\n",
       " ('1/1/2012', '13:00'),\n",
       " ('1/1/2012', '14:00'),\n",
       " ('1/1/2012', '15:00'),\n",
       " ('1/1/2012', '16:00'),\n",
       " ('1/1/2012', '17:00'),\n",
       " ('1/1/2012', '18:00'),\n",
       " ('1/1/2012', '19:00'),\n",
       " ('1/1/2012', '20:00'),\n",
       " ('1/1/2012', '21:00'),\n",
       " ('1/1/2012', '22:00'),\n",
       " ('1/1/2012', '23:00'),\n",
       " ...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile(\"C:\\sparkProjectFolder\\purchase.csv\") \\\n",
    "    .map(lambda line:line.split(\",\")) \\\n",
    "    .filter(lambda line: len(line)>1) \\\n",
    "    .map(lambda line: (line[0],line[1])) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "purchaseSchema =StructType([\n",
    "    StructField(\"Date\",DateType(),True),\n",
    "    StructField(\"Time\",StringType(),True),\n",
    "    StructField(\"City\",StringType(),True),\n",
    "    StructField(\"Item\",StringType(),True),\n",
    "    StructField(\"Total\",FloatType(),True),\n",
    "    StructField(\"Payment\", StringType(),True)\n",
    "])\n",
    "\n",
    "df = sqlContext.read.format('com.databricks.spark.csv').options(header=True, schema=purchaseSchema).load('purchase.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----+-------+\n",
      "|    Date| Time|    City|  Item|Total|Payment|\n",
      "+--------+-----+--------+------+-----+-------+\n",
      "|1/1/2012| 9:00|  Indore|  Milk|   20|   Visa|\n",
      "|1/1/2012|10:00|Banglore|Fruits|  100|   Amex|\n",
      "+--------+-----+--------+------+-----+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows:  5164\n",
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Item: string (nullable = true)\n",
      " |-- Total: string (nullable = true)\n",
      " |-- Payment: string (nullable = true)\n",
      "\n",
      "+-------+------------------+\n",
      "|summary|             Total|\n",
      "+-------+------------------+\n",
      "|  count|              5164|\n",
      "|   mean|          206540.0|\n",
      "| stddev|119269.01805023242|\n",
      "|    min|               100|\n",
      "|    max|             99940|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_rows=df.count()\n",
    "print(\"number of rows: \", num_rows)\n",
    "df.printSchema()\n",
    "df.describe('Total').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "purchaseSchema =StructType([\n",
    "    StructField(\"Date\",DateType(),True),\n",
    "    StructField(\"Time\",StringType(),True),\n",
    "    StructField(\"City\",StringType(),True),\n",
    "    StructField(\"Item\",StringType(),True),\n",
    "    StructField(\"Total\",FloatType(),True),\n",
    "    StructField(\"Payment\", StringType(),True)\n",
    "])\n",
    "\n",
    "df = sqlContext.read.format('com.databricks.spark.csv').options(header=True, schema=purchaseSchema).load('purchase.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----+-------+\n",
      "|    Date| Time|    City|  Item|Total|Payment|\n",
      "+--------+-----+--------+------+-----+-------+\n",
      "|1/1/2012| 9:00|  Indore|  Milk|   20|   Visa|\n",
      "|1/1/2012|10:00|Banglore|Fruits|  100|   Amex|\n",
      "+--------+-----+--------+------+-----+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----+-------+\n",
      "|    Date| Time|    City|  Item|Total|Payment|\n",
      "+--------+-----+--------+------+-----+-------+\n",
      "|1/1/2012| 9:00|  Indore|  Milk|   20|   Visa|\n",
      "|1/1/2012|10:00|Banglore|Fruits|  100|   Amex|\n",
      "|1/1/2012|11:00|  Indore|  Milk|  180|   Visa|\n",
      "|1/1/2012|12:00|Banglore|Fruits|  260|   Amex|\n",
      "|1/1/2012|13:00|  Indore|  Milk|  340|   Visa|\n",
      "|1/1/2012|14:00|Banglore|Fruits|  420|   Amex|\n",
      "|1/1/2012|15:00|  Indore|  Milk|  500|   Visa|\n",
      "|1/1/2012|16:00|Banglore|Fruits|  580|   Amex|\n",
      "|1/1/2012|17:00|  Indore|  Milk|  660|   Visa|\n",
      "|1/1/2012|18:00|Banglore|Fruits|  740|   Amex|\n",
      "+--------+-----+--------+------+-----+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows:  5164\n",
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Item: string (nullable = true)\n",
      " |-- Total: string (nullable = true)\n",
      " |-- Payment: string (nullable = true)\n",
      "\n",
      "+-------+------------------+\n",
      "|summary|             Total|\n",
      "+-------+------------------+\n",
      "|  count|              5164|\n",
      "|   mean|          206540.0|\n",
      "| stddev|119269.01805023242|\n",
      "|    min|               100|\n",
      "|    max|             99940|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_rows=df.count()\n",
    "print(\"number of rows: \", num_rows)\n",
    "df.printSchema()\n",
    "df.describe('Total').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "purchaseSchema =StructType([\n",
    "    StructField(\"tweet_id\",StringType(),True),\n",
    "    StructField(\"author_id\",StringType(),True),\n",
    "    StructField(\"inbound\",StringType(),True),\n",
    "    StructField(\"created_at\",DateType(),True),\n",
    "    StructField(\"text\",StringType(),True),\n",
    "    StructField(\"response_tweet_id\", StringType(),True),\n",
    "    StructField(\"in_response_tweet_id\", StringType(),True)\n",
    "])\n",
    "\n",
    "df = sqlContext.read.format('com.databricks.spark.csv').options(header=True, schema=purchaseSchema).load('twcs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------+--------------------+--------------------+-----------------+-----------------------+\n",
      "|tweet_id| author_id|inbound|          created_at|                text|response_tweet_id|in_response_to_tweet_id|\n",
      "+--------+----------+-------+--------------------+--------------------+-----------------+-----------------------+\n",
      "|       1|sprintcare|  False|Tue Oct 31 22:10:...|@115712 I underst...|                2|                      3|\n",
      "|       2|    115712|   True|Tue Oct 31 22:11:...|@sprintcare and h...|             null|                      1|\n",
      "+--------+----------+-------+--------------------+--------------------+-----------------+-----------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------+--------------------+--------------------+-----------------+-----------------------+\n",
      "|tweet_id| author_id|inbound|          created_at|                text|response_tweet_id|in_response_to_tweet_id|\n",
      "+--------+----------+-------+--------------------+--------------------+-----------------+-----------------------+\n",
      "|       1|sprintcare|  False|Tue Oct 31 22:10:...|@115712 I underst...|                2|                      3|\n",
      "|       2|    115712|   True|Tue Oct 31 22:11:...|@sprintcare and h...|             null|                      1|\n",
      "|       3|    115712|   True|Tue Oct 31 22:08:...|@sprintcare I hav...|                1|                      4|\n",
      "|       4|sprintcare|  False|Tue Oct 31 21:54:...|@115712 Please se...|                3|                      5|\n",
      "|       5|    115712|   True|Tue Oct 31 21:49:...|  @sprintcare I did.|                4|                      6|\n",
      "|       6|sprintcare|  False|Tue Oct 31 21:46:...|@115712 Can you p...|              5,7|                      8|\n",
      "|       8|    115712|   True|Tue Oct 31 21:45:...|@sprintcare is th...|           9,6,10|                   null|\n",
      "|      11|sprintcare|  False|Tue Oct 31 22:10:...|@115713 This is s...|             null|                     12|\n",
      "|      12|    115713|   True|Tue Oct 31 22:04:...|@sprintcare You g...|         11,13,14|                     15|\n",
      "|      15|sprintcare|  False|Tue Oct 31 20:03:...|@115713 We unders...|               12|                     16|\n",
      "+--------+----------+-------+--------------------+--------------------+-----------------+-----------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows:  2966469\n",
      "root\n",
      " |-- tweet_id: string (nullable = true)\n",
      " |-- author_id: string (nullable = true)\n",
      " |-- inbound: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- response_tweet_id: string (nullable = true)\n",
      " |-- in_response_to_tweet_id: string (nullable = true)\n",
      "\n",
      "+-------+--------------------+\n",
      "|summary|             inbound|\n",
      "+-------+--------------------+\n",
      "|  count|             2879535|\n",
      "|   mean|  1465289.6061322398|\n",
      "| stddev|   922298.6833181768|\n",
      "|    min|                    |\n",
      "|    max|장갑을 벗고 맨손으로 누르면 바...|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_rows=df.count()\n",
    "print(\"number of rows: \", num_rows)\n",
    "df.printSchema()\n",
    "df.describe('inbound').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows:  2966469\n",
      "root\n",
      " |-- tweet_id: string (nullable = true)\n",
      " |-- author_id: string (nullable = true)\n",
      " |-- inbound: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- response_tweet_id: string (nullable = true)\n",
      " |-- in_response_to_tweet_id: string (nullable = true)\n",
      "\n",
      "+-------+--------------------+\n",
      "|summary|            tweet_id|\n",
      "+-------+--------------------+\n",
      "|  count|             2966449|\n",
      "|   mean|1.6419553665565941E7|\n",
      "| stddev|  9.55187119471662E9|\n",
      "|    min|                 ...|\n",
      "|    max|                  🧠|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_rows=df.count()\n",
    "print(\"number of rows: \", num_rows)\n",
    "df.printSchema()\n",
    "df.describe('tweet_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "purchaseSchema =StructType([\n",
    "    StructField(\"Date\",DateType(),True),\n",
    "    StructField(\"Time\",StringType(),True),\n",
    "    StructField(\"City\",StringType(),True),\n",
    "    StructField(\"Item\",StringType(),True),\n",
    "    StructField(\"Total\",FloatType(),True),\n",
    "    StructField(\"Payment\", StringType(),True)\n",
    "])\n",
    "\n",
    "df = sqlContext.read.format('com.databricks.spark.csv').options(header=True, schema=purchaseSchema).load('purchase.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----+-------+\n",
      "|    Date| Time|    City|  Item|Total|Payment|\n",
      "+--------+-----+--------+------+-----+-------+\n",
      "|1/1/2012| 9:00|  Indore|  Milk|   20|   Visa|\n",
      "|1/1/2012|10:00|Banglore|Fruits|  100|   Amex|\n",
      "|1/1/2012|11:00|  Indore|  Milk|  180|   Visa|\n",
      "|1/1/2012|12:00|Banglore|Fruits|  260|   Amex|\n",
      "|1/1/2012|13:00|  Indore|  Milk|  340|   Visa|\n",
      "|1/1/2012|14:00|Banglore|Fruits|  420|   Amex|\n",
      "|1/1/2012|15:00|  Indore|  Milk|  500|   Visa|\n",
      "|1/1/2012|16:00|Banglore|Fruits|  580|   Amex|\n",
      "|1/1/2012|17:00|  Indore|  Milk|  660|   Visa|\n",
      "|1/1/2012|18:00|Banglore|Fruits|  740|   Amex|\n",
      "+--------+-----+--------+------+-----+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|    City|Total|\n",
      "+--------+-----+\n",
      "|  Indore|   20|\n",
      "|Banglore|  100|\n",
      "|  Indore|  180|\n",
      "|Banglore|  260|\n",
      "|  Indore|  340|\n",
      "|Banglore|  420|\n",
      "|  Indore|  500|\n",
      "|Banglore|  580|\n",
      "|  Indore|  660|\n",
      "|Banglore|  740|\n",
      "+--------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- Total: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newDataframe = df.select(df['City'],df['Total'])\n",
    "newDataframe.show(10)\n",
    "newDataframe.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "|    City|(Total + 10)|\n",
      "+--------+------------+\n",
      "|  Indore|        30.0|\n",
      "|Banglore|       110.0|\n",
      "|  Indore|       190.0|\n",
      "|Banglore|       270.0|\n",
      "|  Indore|       350.0|\n",
      "|Banglore|       430.0|\n",
      "|  Indore|       510.0|\n",
      "|Banglore|       590.0|\n",
      "|  Indore|       670.0|\n",
      "|Banglore|       750.0|\n",
      "+--------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df['City'],df['Total']+10).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----+-------+\n",
      "|    Date| Time|    City|  Item|Total|Payment|\n",
      "+--------+-----+--------+------+-----+-------+\n",
      "|1/1/2012|22:00|Banglore|Fruits| 1060|   Amex|\n",
      "|1/1/2012|23:00|  Indore|  Milk| 1140|   Visa|\n",
      "|1/1/2012| 0:00|Banglore|Fruits| 1220|   Amex|\n",
      "|1/1/2012| 1:00|  Indore|  Milk| 1300|   Visa|\n",
      "|1/1/2012| 2:00|Banglore|Fruits| 1380|   Amex|\n",
      "|1/1/2012| 3:00|  Indore|  Milk| 1460|   Visa|\n",
      "|1/1/2012| 4:00|Banglore|Fruits| 1540|   Amex|\n",
      "|1/1/2012| 5:00|  Indore|  Milk| 1620|   Visa|\n",
      "|1/1/2012| 6:00|Banglore|Fruits| 1700|   Amex|\n",
      "|1/1/2012| 7:00|  Indore|  Milk| 1780|   Visa|\n",
      "+--------+-----+--------+------+-----+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['Total']>1000).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----+-------+\n",
      "|    Date| Time|    City|  Item|Total|Payment|\n",
      "+--------+-----+--------+------+-----+-------+\n",
      "|1/1/2012|20:00|Banglore|Fruits|  900|   Amex|\n",
      "|1/1/2012|16:00|Banglore|Fruits| 2500|   Amex|\n",
      "|1/1/2012|22:00|Banglore|Fruits| 1060|   Amex|\n",
      "|1/1/2012|10:00|Banglore|Fruits|  100|   Amex|\n",
      "|1/1/2012| 0:00|Banglore|Fruits| 1220|   Amex|\n",
      "|1/1/2012|12:00|Banglore|Fruits|  260|   Amex|\n",
      "|1/1/2012| 2:00|Banglore|Fruits| 1380|   Amex|\n",
      "|1/1/2012|14:00|Banglore|Fruits|  420|   Amex|\n",
      "|1/1/2012| 4:00|Banglore|Fruits| 1540|   Amex|\n",
      "|1/1/2012|16:00|Banglore|Fruits|  580|   Amex|\n",
      "+--------+-----+--------+------+-----+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sortedByCity =df.orderBy('City').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|    City|count|\n",
      "+--------+-----+\n",
      "|Banglore| 2582|\n",
      "|  Indore| 2582|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numCity = df.groupBy(\"City\").count()\n",
    "numCity.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "purchaseSchema =StructType([\n",
    "    StructField(\"Date\",DateType(),True),\n",
    "    StructField(\"Time\",StringType(),True),\n",
    "    StructField(\"City\",StringType(),True),\n",
    "    StructField(\"Item\",StringType(),True),\n",
    "    StructField(\"Total\",FloatType(),True),\n",
    "    StructField(\"Payment\", StringType(),True)\n",
    "])\n",
    "\n",
    "df = sqlContext.read.format('com.databricks.spark.csv').options(header=True, schema=purchaseSchema).load('purchases.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+\n",
      "|Date\tTime\tCity\tItem\tTotal\tPayment|\n",
      "+---------------------------------+\n",
      "|             2012-01-01\t09:00\t...|\n",
      "|             2012-01-01\t09:00\t...|\n",
      "|             2012-01-01\t09:00\t...|\n",
      "|             2012-01-01\t09:00\t...|\n",
      "|             2012-01-01\t09:00\t...|\n",
      "|             2012-01-01\t09:00\t...|\n",
      "|             2012-01-01\t09:00\t...|\n",
      "|             2012-01-01\t09:00\t...|\n",
      "|             2012-01-01\t09:00\t...|\n",
      "|             2012-01-01\t09:00\t...|\n",
      "+---------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+\n",
      "|Date\tTime\tCity\tItem\tTotal\tPayment|\n",
      "+---------------------------------+\n",
      "|             2012-01-01\t09:00\t...|\n",
      "|             2012-01-01\t09:00\t...|\n",
      "|             2012-01-01\t09:00\t...|\n",
      "+---------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show (3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+-----+\n",
      "|Date\tTime\tCity\tItem\tTotal\tPayment|index|\n",
      "+---------------------------------+-----+\n",
      "|             2012-01-01\t09:00\t...|    0|\n",
      "|             2012-01-01\t09:00\t...|    1|\n",
      "|             2012-01-01\t09:00\t...|    2|\n",
      "|             2012-01-01\t09:00\t...|    3|\n",
      "|             2012-01-01\t09:00\t...|    4|\n",
      "|             2012-01-01\t09:00\t...|    5|\n",
      "|             2012-01-01\t09:00\t...|    6|\n",
      "+---------------------------------+-----+\n",
      "only showing top 7 rows\n",
      "\n",
      "+---------------------------------+-----+\n",
      "|Date\tTime\tCity\tItem\tTotal\tPayment|index|\n",
      "+---------------------------------+-----+\n",
      "|             2012-01-01\t09:00\t...|    2|\n",
      "|             2012-01-01\t09:00\t...|    3|\n",
      "|             2012-01-01\t09:00\t...|    4|\n",
      "+---------------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "newPurchaseDataframe=df.withColumn(\"index\", monotonically_increasing_id())\n",
    "newPurchaseDataframe.show(7)\n",
    "row2Till4=newPurchaseDataframe.filter((newPurchaseDataframe['index']>=2)&(newPurchaseDataframe['index']<=4))\n",
    "row2Till4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`Total`' given input columns: [purchasesql.Date\\tTime\\tCity\\tItem\\tTotal\\tPayment]; line 1 pos 7;\\n'Project ['Total, 'Payment]\\n+- SubqueryAlias purchasesql\\n   +- Relation[Date\\tTime\\tCity\\tItem\\tTotal\\tPayment#732] csv\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\apache-spark\\spark-2.3.2-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apache-spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o28.sql.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`Total`' given input columns: [purchasesql.Date\tTime\tCity\tItem\tTotal\tPayment]; line 1 pos 7;\n'Project ['Total, 'Payment]\n+- SubqueryAlias purchasesql\n   +- Relation[Date\tTime\tCity\tItem\tTotal\tPayment#732] csv\n\r\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:92)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:89)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:95)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:95)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:107)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:107)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:106)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:118)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:122)\r\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n\tat scala.collection.immutable.List.foreach(List.scala:381)\r\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\r\n\tat scala.collection.immutable.List.map(List.scala:285)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:122)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:127)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:127)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:95)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:89)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:84)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:84)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:92)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-f98e0e845aff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateOrReplaceTempView\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"purchaseSql\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0manotherNewDataframe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"SELECT Total, Payment FROM purchaseSql\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0manotherNewDataframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apache-spark\\spark-2.3.2-bin-hadoop2.7\\python\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36msql\u001b[1;34m(self, sqlQuery)\u001b[0m\n\u001b[0;32m    708\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    709\u001b[0m         \"\"\"\n\u001b[1;32m--> 710\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    711\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    712\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apache-spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apache-spark\\spark-2.3.2-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[0;32m     68\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: \"cannot resolve '`Total`' given input columns: [purchasesql.Date\\tTime\\tCity\\tItem\\tTotal\\tPayment]; line 1 pos 7;\\n'Project ['Total, 'Payment]\\n+- SubqueryAlias purchasesql\\n   +- Relation[Date\\tTime\\tCity\\tItem\\tTotal\\tPayment#732] csv\\n\""
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"purchaseSql\")\n",
    "anotherNewDataframe=spark.sql(\"SELECT Total, Payment FROM purchaseSql\")\n",
    "anotherNewDataframe.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`City`' given input columns: [purchasesql.Date\\tTime\\tCity\\tItem\\tTotal\\tPayment]; line 1 pos 35;\\n'Sort ['City ASC NULLS FIRST], true\\n+- Project [Date\\tTime\\tCity\\tItem\\tTotal\\tPayment#732]\\n   +- SubqueryAlias purchasesql\\n      +- Relation[Date\\tTime\\tCity\\tItem\\tTotal\\tPayment#732] csv\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\apache-spark\\spark-2.3.2-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apache-spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o28.sql.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`City`' given input columns: [purchasesql.Date\tTime\tCity\tItem\tTotal\tPayment]; line 1 pos 35;\n'Sort ['City ASC NULLS FIRST], true\n+- Project [Date\tTime\tCity\tItem\tTotal\tPayment#732]\n   +- SubqueryAlias purchasesql\n      +- Relation[Date\tTime\tCity\tItem\tTotal\tPayment#732] csv\n\r\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:92)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:89)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:95)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:95)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:107)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:107)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:106)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:118)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:122)\r\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:122)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:127)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:127)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:95)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:89)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:84)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:84)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:92)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-9f0f2ec7172c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0morderByCity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"SELECT * FROM purchaseSql ORDER BY City\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0morderByCity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apache-spark\\spark-2.3.2-bin-hadoop2.7\\python\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36msql\u001b[1;34m(self, sqlQuery)\u001b[0m\n\u001b[0;32m    708\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    709\u001b[0m         \"\"\"\n\u001b[1;32m--> 710\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    711\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    712\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apache-spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apache-spark\\spark-2.3.2-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[0;32m     68\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: \"cannot resolve '`City`' given input columns: [purchasesql.Date\\tTime\\tCity\\tItem\\tTotal\\tPayment]; line 1 pos 35;\\n'Sort ['City ASC NULLS FIRST], true\\n+- Project [Date\\tTime\\tCity\\tItem\\tTotal\\tPayment#732]\\n   +- SubqueryAlias purchasesql\\n      +- Relation[Date\\tTime\\tCity\\tItem\\tTotal\\tPayment#732] csv\\n\""
     ]
    }
   ],
   "source": [
    "orderByCity=spark.sql(\"SELECT * FROM purchaseSql ORDER BY City\")\n",
    "orderByCity.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`Total`' given input columns: [purchasesql.Date\\tTime\\tCity\\tItem\\tTotal\\tPayment]; line 1 pos 32;\\n'Sort ['Payment ASC NULLS FIRST], true\\n+- 'Project [*]\\n   +- 'Filter ('Total > 200)\\n      +- SubqueryAlias purchasesql\\n         +- Relation[Date\\tTime\\tCity\\tItem\\tTotal\\tPayment#732] csv\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\apache-spark\\spark-2.3.2-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apache-spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o28.sql.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`Total`' given input columns: [purchasesql.Date\tTime\tCity\tItem\tTotal\tPayment]; line 1 pos 32;\n'Sort ['Payment ASC NULLS FIRST], true\n+- 'Project [*]\n   +- 'Filter ('Total > 200)\n      +- SubqueryAlias purchasesql\n         +- Relation[Date\tTime\tCity\tItem\tTotal\tPayment#732] csv\n\r\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:92)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:89)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:95)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:95)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:107)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:107)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:106)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:118)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:127)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:127)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:95)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:89)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:84)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\r\n\tat scala.collection.immutable.List.foreach(List.scala:381)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\r\n\tat scala.collection.immutable.List.foreach(List.scala:381)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:84)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:92)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-41a6eed6b05d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfilterandSortSQL\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"SELECT * FROM purchaseSql WHERE Total>200 ORDER BY Payment\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mfilterandSortSQL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apache-spark\\spark-2.3.2-bin-hadoop2.7\\python\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36msql\u001b[1;34m(self, sqlQuery)\u001b[0m\n\u001b[0;32m    708\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    709\u001b[0m         \"\"\"\n\u001b[1;32m--> 710\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    711\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    712\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apache-spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apache-spark\\spark-2.3.2-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[0;32m     68\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: \"cannot resolve '`Total`' given input columns: [purchasesql.Date\\tTime\\tCity\\tItem\\tTotal\\tPayment]; line 1 pos 32;\\n'Sort ['Payment ASC NULLS FIRST], true\\n+- 'Project [*]\\n   +- 'Filter ('Total > 200)\\n      +- SubqueryAlias purchasesql\\n         +- Relation[Date\\tTime\\tCity\\tItem\\tTotal\\tPayment#732] csv\\n\""
     ]
    }
   ],
   "source": [
    "filterandSortSQL=spark.sql(\"SELECT * FROM purchaseSql WHERE Total>200 ORDER BY Payment\")\n",
    "filterandSortSQL.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " from pyspark.sql.types import *\n",
    "appname=\"data preprocessing in Spark\"\n",
    "spark= SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(appname)\\\n",
    "    .config(\"spark.some.config.option\",\"some-value\")\\\n",
    "    .getOrCreate()\n",
    "flightSchema=StructType([\n",
    "    StructField(\"DayofMonth\", IntegerType(),True),\n",
    "    StructField(\"DayOfWeek\",IntegerType(),True),\n",
    "    StructField(\"Carrier\",StringType(),True),\n",
    "    StructField(\"OriginAirportID\",IntegerType(),True),\n",
    "    StructField(\"DestAirportID\",IntegerType(),True),\n",
    "    StructField(\"DepDelay\", IntegerType(), True),\n",
    "    StructField(\"ArrDelay\",IntegerType(),True)\n",
    "])\n",
    "flights= sqlContext.read.format('com.databricks.spark.csv').options(header=True, schema=flightSchema).load('raw-flight-data.csv')\n",
    "flights.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airportSchema=StructType([\n",
    "    StructField(\"airport_id\",IntegerType(),False),\n",
    "    StructField(\"city\",StringType(),False),\n",
    "    StructField(\"state\",StringType(),False),\n",
    "    StructField(\"name\",StringType(),False)\n",
    "])\n",
    "airports=sqlContext.read.format('com.databricks.spark.csv').options(header=True, schema=airportSchema).load('airports.csv')\n",
    "airports.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsByOrigin=flights.join(airports,flights.OriginAirportID==airports.airport_id).groupBy(\"city\").count()\n",
    "flightsByOrigin.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1=flights.count()\n",
    "print(\"number of orignal data rows\", n1)\n",
    "n2=flights.dropDuplicates().count()\n",
    "print(\"number of data rows after deletion\", n2)\n",
    "n3=n1-n2\n",
    "print(\"number of rows deleted\",n3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+---------------+-------------+--------+--------+\n",
      "|DayofMonth|DayOfWeek|Carrier|OriginAirportID|DestAirportID|DepDelay|ArrDelay|\n",
      "+----------+---------+-------+---------------+-------------+--------+--------+\n",
      "|        19|        5|     DL|          11433|        13303|      -3|       1|\n",
      "|        19|        5|     DL|          14869|        12478|       0|      -8|\n",
      "|        19|        5|     DL|          14057|        14869|      -4|     -15|\n",
      "+----------+---------+-------+---------------+-------------+--------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "appName=\"Regression in Spark\"\n",
    "spark=SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(appName) \\\n",
    "    .config(\"spark.some.config.option\",\"some-value\") \\\n",
    "    .getOrCreate()\n",
    "flightSchema=StructType([\n",
    "    StructField(\"DayofMonth\", IntegerType(),True),\n",
    "    StructField(\"DayOfWeek\",IntegerType(),True),\n",
    "    StructField(\"Carrier\",StringType(),True),\n",
    "    StructField(\"OriginAirportID\",IntegerType(),True),\n",
    "    StructField(\"DestAirportID\",IntegerType(),True),\n",
    "    StructField(\"DepDelay\", IntegerType(), True),\n",
    "    StructField(\"ArrDelay\",IntegerType(),True)\n",
    "])\n",
    "flights= spark.read.csv('flights.csv',schema=flightSchema,header=True)\n",
    "flights.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------------+-------------+--------+--------+\n",
      "|DayofMonth|DayofWeek|OriginAirportID|DestAirportID|DepDelay|ArrDelay|\n",
      "+----------+---------+---------------+-------------+--------+--------+\n",
      "|        19|        5|          11433|        13303|      -3|       1|\n",
      "|        19|        5|          14869|        12478|       0|      -8|\n",
      "|        19|        5|          14057|        14869|      -4|     -15|\n",
      "+----------+---------+---------------+-------------+--------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=flights.select(\"DayofMonth\",\"DayofWeek\",\"OriginAirportID\",\"DestAirportID\",\"DepDelay\",\"ArrDelay\")\n",
    "data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data rows: 1892523 ;Testing data rows: 809695\n"
     ]
    }
   ],
   "source": [
    "dividedData=data.randomSplit([0.7,0.3])\n",
    "trainingData=dividedData[0]\n",
    "testingData=dividedData[1]\n",
    "train_rows=trainingData.count()\n",
    "test_rows=testingData.count()\n",
    "print(\"Training data rows:\",train_rows,\";Testing data rows:\",test_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+-----+\n",
      "|features                      |label|\n",
      "+------------------------------+-----+\n",
      "|[1.0,1.0,10140.0,10397.0,-2.0]|-17  |\n",
      "|[1.0,1.0,10140.0,10821.0,8.0] |-9   |\n",
      "|[1.0,1.0,10140.0,11259.0,-2.0]|-14  |\n",
      "+------------------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembler=VectorAssembler(inputCols=[\"DayofMonth\",\"DayofWeek\",\"OriginAirportID\",\"DestAirportID\",\"DepDelay\"],outputCol=\"features\")\n",
    "trainingDataFinal=assembler.transform(trainingData).select(col(\"features\"),(col(\"ArrDelay\").cast(\"Int\").alias(\"label\")))\n",
    "trainingDataFinal.show(truncate=False, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression Model trained\n"
     ]
    }
   ],
   "source": [
    "algorithm=LinearRegression(labelCol=\"label\",featuresCol=\"features\",maxIter=10,regParam=0.3)\n",
    "model=algorithm.fit(trainingDataFinal)\n",
    "print(\"Regression Model trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+---------+\n",
      "|features                      |trueLabel|\n",
      "+------------------------------+---------+\n",
      "|[1.0,1.0,10140.0,10397.0,-4.0]|-11      |\n",
      "|[1.0,1.0,10140.0,11259.0,-3.0]|-11      |\n",
      "+------------------------------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testingDataFinal=assembler.transform(testingData).select(col(\"features\"),(col(\"ArrDelay\")).cast(\"Int\").alias(\"trueLabel\"))\n",
    "testingDataFinal.show(truncate=False,n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------------------+\n",
      "|            features|trueLabel|         prediction|\n",
      "+--------------------+---------+-------------------+\n",
      "|[1.0,1.0,10140.0,...|      -11| -7.567268216025724|\n",
      "|[1.0,1.0,10140.0,...|      -11|  -6.76411741274002|\n",
      "|[1.0,1.0,10140.0,...|      -11|  -4.76935865656568|\n",
      "|[1.0,1.0,10140.0,...|      -12|-3.7719792784785096|\n",
      "|[1.0,1.0,10140.0,...|       -8| -7.768932455523765|\n",
      "|[1.0,1.0,10140.0,...|       -6| -4.776794321262255|\n",
      "|[1.0,1.0,10140.0,...|      -13|-13.754560663082527|\n",
      "|[1.0,1.0,10140.0,...|       -9| -4.778146260297996|\n",
      "|[1.0,1.0,10140.0,...|       68|  82.99123901137297|\n",
      "|[1.0,1.0,10140.0,...|       13|   11.9760895740311|\n",
      "+--------------------+---------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction=model.transform(testingDataFinal)\n",
    "prediction.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Square:  13.24648187612213\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator=RegressionEvaluator(labelCol=\"trueLabel\",predictionCol=\"prediction\",metricName=\"rmse\")\n",
    "rmse=evaluator.evaluate(prediction)\n",
    "print(\"Root Mean Square: \",rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+---------------+-------------+--------+--------+\n",
      "|DayofMonth|DayOfWeek|Carrier|OriginAirportID|DestAirportID|DepDelay|ArrDelay|\n",
      "+----------+---------+-------+---------------+-------------+--------+--------+\n",
      "|        19|        5|     DL|          11433|        13303|      -3|       1|\n",
      "|        19|        5|     DL|          14869|        12478|       0|      -8|\n",
      "|        19|        5|     DL|          14057|        14869|      -4|     -15|\n",
      "+----------+---------+-------+---------------+-------------+--------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "appName=\"Classification in Spark\"\n",
    "spark=SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(appName) \\\n",
    "    .config(\"spark.some.config.option\",\"some-value\") \\\n",
    "    .getOrCreate()\n",
    "flightSchema=StructType([\n",
    "    StructField(\"DayofMonth\", IntegerType(),True),\n",
    "    StructField(\"DayOfWeek\",IntegerType(),True),\n",
    "    StructField(\"Carrier\",StringType(),True),\n",
    "    StructField(\"OriginAirportID\",IntegerType(),True),\n",
    "    StructField(\"DestAirportID\",IntegerType(),True),\n",
    "    StructField(\"DepDelay\", IntegerType(), True),\n",
    "    StructField(\"ArrDelay\",IntegerType(),True)\n",
    "])\n",
    "csv= spark.read.csv('flights.csv',schema=flightSchema,header=True)\n",
    "csv.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------------+-------------+--------+----+\n",
      "|DayOfMonth|DayOfWeek|OriginAirportID|DestAirportID|DepDelay|Late|\n",
      "+----------+---------+---------------+-------------+--------+----+\n",
      "|        19|        5|          11433|        13303|      -3|   0|\n",
      "|        19|        5|          14869|        12478|       0|   0|\n",
      "|        19|        5|          14057|        14869|      -4|   0|\n",
      "+----------+---------+---------------+-------------+--------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = csv.select(\"DayOfMonth\",\"DayOfWeek\",\"OriginAirportID\",\"DestAirportID\",\"DepDelay\",((col(\"ArrDelay\")>15).cast(\"Int\").alias(\"Late\")))\n",
    "data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data rows: 1890527 ;Testing data rows: 811691\n"
     ]
    }
   ],
   "source": [
    "dividedData=data.randomSplit([0.7,0.3])\n",
    "trainingData=dividedData[0]\n",
    "testingData=dividedData[1]\n",
    "train_rows=trainingData.count()\n",
    "test_rows=testingData.count()\n",
    "print(\"Training data rows:\",train_rows,\";Testing data rows:\",test_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+-----+\n",
      "|features                      |label|\n",
      "+------------------------------+-----+\n",
      "|[1.0,1.0,10140.0,10397.0,-2.0]|0    |\n",
      "|[1.0,1.0,10140.0,10821.0,8.0] |0    |\n",
      "+------------------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembler=VectorAssembler(inputCols=[\"DayOfMonth\",\"DayOfWeek\",\"OriginAirportID\",\"DestAirportID\",\"DepDelay\"],outputCol=\"features\")\n",
    "trainingDataFinal=assembler.transform(trainingData).select(col(\"features\"),col(\"Late\").alias(\"label\"))\n",
    "trainingDataFinal.show(truncate=False,n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier model is trained\n"
     ]
    }
   ],
   "source": [
    "classifier=LogisticRegression(labelCol=\"label\",featuresCol=\"features\",maxIter=10,regParam=0.3)\n",
    "model = classifier.fit(trainingDataFinal)\n",
    "print(\"Classifier model is trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|            features|trueLabel|\n",
      "+--------------------+---------+\n",
      "|[1.0,1.0,10140.0,...|        0|\n",
      "|[1.0,1.0,10140.0,...|        0|\n",
      "|[1.0,1.0,10140.0,...|        0|\n",
      "+--------------------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testingDataFinal=assembler.transform(testingData).select(col(\"features\"),col(\"Late\").alias(\"trueLabel\"))\n",
    "testingDataFinal.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+----------+----------------------------------------+---------+\n",
      "|features                      |prediction|probability                             |trueLabel|\n",
      "+------------------------------+----------+----------------------------------------+---------+\n",
      "|[1.0,1.0,10140.0,10397.0,-4.0]|0.0       |[0.8311925160292101,0.16880748397079004]|0        |\n",
      "|[1.0,1.0,10140.0,11259.0,-1.0]|0.0       |[0.8256156254495102,0.17438437455048977]|0        |\n",
      "|[1.0,1.0,10140.0,11259.0,0.0] |0.0       |[0.8235962506286986,0.1764037493713014] |0        |\n",
      "+------------------------------+----------+----------------------------------------+---------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+------------------------------+---------+----------------------------------------+----------------------------------------+----------+\n",
      "|features                      |trueLabel|rawPrediction                           |probability                             |prediction|\n",
      "+------------------------------+---------+----------------------------------------+----------------------------------------+----------+\n",
      "|[1.0,1.0,10140.0,10397.0,-4.0]|0        |[1.594102518430892,-1.594102518430892]  |[0.8311925160292101,0.16880748397079004]|0.0       |\n",
      "|[1.0,1.0,10140.0,11259.0,-1.0]|0        |[1.5548674087289198,-1.5548674087289198]|[0.8256156254495102,0.17438437455048977]|0.0       |\n",
      "|[1.0,1.0,10140.0,11259.0,0.0] |0        |[1.5409050244393123,-1.5409050244393123]|[0.8235962506286986,0.1764037493713014] |0.0       |\n",
      "+------------------------------+---------+----------------------------------------+----------------------------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction=model.transform(testingDataFinal)\n",
    "predictionFinal=prediction.select(\"features\",\"prediction\",\"probability\",\"trueLabel\")\n",
    "predictionFinal.show(truncate=False, n=3)\n",
    "prediction.show(truncate=False, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct prediction: 669215 ,total data: 811691 ,accuracy: 0.8244701493548653\n"
     ]
    }
   ],
   "source": [
    "correctPrediction=predictionFinal.filter(predictionFinal['prediction']==predictionFinal['trueLabel']).count()\n",
    "totalData=predictionFinal.count()\n",
    "print(\"correct prediction:\", correctPrediction,\",total data:\", totalData,\",accuracy:\",correctPrediction/totalData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model is trainied\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "model2= RandomForestClassifier(numTrees=3, maxDepth=5, seed=42, labelCol=\"label\",featuresCol=\"features\")\n",
    "model2=model2.fit(trainingDataFinal)\n",
    "print(\"model is trainied\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+----------+----------------------------------------+---------+\n",
      "|features                      |prediction|probability                             |trueLabel|\n",
      "+------------------------------+----------+----------------------------------------+---------+\n",
      "|[1.0,1.0,10140.0,10397.0,-4.0]|0.0       |[0.9519669276250302,0.04803307237496979]|0        |\n",
      "|[1.0,1.0,10140.0,11259.0,-1.0]|0.0       |[0.9453619724639896,0.05463802753601036]|0        |\n",
      "|[1.0,1.0,10140.0,11259.0,0.0] |0.0       |[0.9453619724639896,0.05463802753601036]|0        |\n",
      "+------------------------------+----------+----------------------------------------+---------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+------------------------------+---------+----------------------------------------+----------------------------------------+----------+\n",
      "|features                      |trueLabel|rawPrediction                           |probability                             |prediction|\n",
      "+------------------------------+---------+----------------------------------------+----------------------------------------+----------+\n",
      "|[1.0,1.0,10140.0,10397.0,-4.0]|0        |[2.8559007828750906,0.14409921712490936]|[0.9519669276250302,0.04803307237496979]|0.0       |\n",
      "|[1.0,1.0,10140.0,11259.0,-1.0]|0        |[2.836085917391969,0.1639140826080311]  |[0.9453619724639896,0.05463802753601036]|0.0       |\n",
      "|[1.0,1.0,10140.0,11259.0,0.0] |0        |[2.836085917391969,0.1639140826080311]  |[0.9453619724639896,0.05463802753601036]|0.0       |\n",
      "+------------------------------+---------+----------------------------------------+----------------------------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction=model2.transform(testingDataFinal)\n",
    "predictionFinal=prediction.select(\"features\",\"prediction\",\"probability\",\"trueLabel\")\n",
    "predictionFinal.show(truncate=False, n=3)\n",
    "prediction.show(truncate=False, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct prediction: 751978 ,total data: 811691 ,accuracy: 0.9264338276511628\n"
     ]
    }
   ],
   "source": [
    "correctPrediction=predictionFinal.filter(predictionFinal['prediction']==predictionFinal['trueLabel']).count()\n",
    "totalData=predictionFinal.count()\n",
    "print(\"correct prediction:\", correctPrediction,\",total data:\", totalData,\",accuracy:\",correctPrediction/totalData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF,Tokenizer, StopWordsRemover\n",
    "appName=\"Sentiment Analysis in Spark\"\n",
    "spark=SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(appName) \\\n",
    "    .config(\"spark.some.config.option\",\"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+---------------+---------------------------------+\n",
      "|ItemID|Sentiment|SentimentSource|SentimentText                    |\n",
      "+------+---------+---------------+---------------------------------+\n",
      "|1038  |1        |Sentiment140   |that film is fantastic #brilliant|\n",
      "|1804  |1        |Sentiment140   |this music is really bad #myband |\n",
      "|1693  |0        |Sentiment140   |winter is terrible #thumbs-down  |\n",
      "+------+---------+---------------+---------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_csv=spark.read.csv('tweets.csv',inferSchema=True,header=True)\n",
    "tweets_csv.show(truncate=False,n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+-----+\n",
      "|SentimentText                    |label|\n",
      "+---------------------------------+-----+\n",
      "|that film is fantastic #brilliant|1    |\n",
      "|this music is really bad #myband |1    |\n",
      "|winter is terrible #thumbs-down  |0    |\n",
      "|this game is awful #nightmare    |0    |\n",
      "|I love jam #loveit               |1    |\n",
      "+---------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=tweets_csv.select(\"SentimentText\",col(\"Sentiment\").cast(\"Int\").alias(\"label\"))\n",
    "data.show(truncate=False,n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data rows: 1340 ; Testing Data Rows:  592\n"
     ]
    }
   ],
   "source": [
    "dividedData=data.randomSplit([0.7,0.3])\n",
    "trainingData = dividedData[0]\n",
    "testingData = dividedData[1]\n",
    "train_rows=trainingData.count()\n",
    "test_rows=testingData.count()\n",
    "print(\"Training data rows:\", train_rows,\"; Testing Data Rows: \",test_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+-----+---------------------------------------+\n",
      "|SentimentText                    |label|SentimentWords                         |\n",
      "+---------------------------------+-----+---------------------------------------+\n",
      "|I adore cheese #brilliant        |1    |[i, adore, cheese, #brilliant]         |\n",
      "|I adore cheese #favorite         |1    |[i, adore, cheese, #favorite]          |\n",
      "|I adore cheese #thumbs-up        |1    |[i, adore, cheese, #thumbs-up]         |\n",
      "|I adore cheese #toptastic        |1    |[i, adore, cheese, #toptastic]         |\n",
      "|I adore classical music #bestever|1    |[i, adore, classical, music, #bestever]|\n",
      "+---------------------------------+-----+---------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer=Tokenizer(inputCol=\"SentimentText\",outputCol=\"SentimentWords\")\n",
    "tokenizedTrain=tokenizer.transform(trainingData)\n",
    "tokenizedTrain.show(truncate=False,n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+-----+---------------------------------------+------------------------------------+\n",
      "|SentimentText                    |label|SentimentWords                         |MeaningfullWords                    |\n",
      "+---------------------------------+-----+---------------------------------------+------------------------------------+\n",
      "|I adore cheese #brilliant        |1    |[i, adore, cheese, #brilliant]         |[adore, cheese, #brilliant]         |\n",
      "|I adore cheese #favorite         |1    |[i, adore, cheese, #favorite]          |[adore, cheese, #favorite]          |\n",
      "|I adore cheese #thumbs-up        |1    |[i, adore, cheese, #thumbs-up]         |[adore, cheese, #thumbs-up]         |\n",
      "|I adore cheese #toptastic        |1    |[i, adore, cheese, #toptastic]         |[adore, cheese, #toptastic]         |\n",
      "|I adore classical music #bestever|1    |[i, adore, classical, music, #bestever]|[adore, classical, music, #bestever]|\n",
      "+---------------------------------+-----+---------------------------------------+------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swr=StopWordsRemover(inputCol=tokenizer.getOutputCol(),outputCol=\"MeaningfullWords\")\n",
    "SwRemovedTrain = swr.transform(tokenizedTrain)\n",
    "SwRemovedTrain.show(truncate=False,n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------------------------+------------------------------------------------------+\n",
      "|label|MeaningfullWords                    |features                                              |\n",
      "+-----+------------------------------------+------------------------------------------------------+\n",
      "|1    |[adore, cheese, #brilliant]         |(262144,[61111,65702,69876],[1.0,1.0,1.0])            |\n",
      "|1    |[adore, cheese, #favorite]          |(262144,[65702,69876,156543],[1.0,1.0,1.0])           |\n",
      "|1    |[adore, cheese, #thumbs-up]         |(262144,[3984,65702,69876],[1.0,1.0,1.0])             |\n",
      "|1    |[adore, cheese, #toptastic]         |(262144,[65702,69876,82232],[1.0,1.0,1.0])            |\n",
      "|1    |[adore, classical, music, #bestever]|(262144,[63443,69876,108823,144106],[1.0,1.0,1.0,1.0])|\n",
      "+-----+------------------------------------+------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashTF=HashingTF(inputCol=swr.getOutputCol(),outputCol=\"features\")\n",
    "numericTrainData=hashTF.transform(SwRemovedTrain).select('label','MeaningfullWords','features')\n",
    "numericTrainData.show(truncate=False,n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is done\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(labelCol=\"label\",featuresCol=\"features\",maxIter=10,regParam=0.01)\n",
    "model=lr.fit(numericTrainData)\n",
    "print(\"Training is done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------------+-------------------------------------------+\n",
      "|label|MeaningfullWords          |features                                   |\n",
      "+-----+--------------------------+-------------------------------------------+\n",
      "|1    |[adore, cheese, #bestever]|(262144,[65702,69876,108823],[1.0,1.0,1.0])|\n",
      "|1    |[adore, cheese, #loveit]  |(262144,[65702,65728,69876],[1.0,1.0,1.0]) |\n",
      "+-----+--------------------------+-------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizedTest=tokenizer.transform(testingData)\n",
    "SwRemovedTest=swr.transform(tokenizedTest)\n",
    "numericTest = hashTF.transform(SwRemovedTest).select('label','MeaningfullWords','features')\n",
    "numericTest.show(truncate=False, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+----------+-----+\n",
      "|MeaningfullWords                     |prediction|Label|\n",
      "+-------------------------------------+----------+-----+\n",
      "|[adore, cheese, #bestever]           |1.0       |1    |\n",
      "|[adore, cheese, #loveit]             |1.0       |1    |\n",
      "|[adore, classical, music, #brilliant]|1.0       |1    |\n",
      "|[adore, classical, music, #thumbs-up]|1.0       |1    |\n",
      "+-------------------------------------+----------+-----+\n",
      "only showing top 4 rows\n",
      "\n",
      "correct prediction: 578 ,total data: 592 ,accuracy: 0.9763513513513513\n"
     ]
    }
   ],
   "source": [
    "prediction=model.transform(numericTest)\n",
    "predictionFinal=prediction.select(\"MeaningfullWords\",\"prediction\",\"Label\")\n",
    "predictionFinal.show(n=4,truncate=False)\n",
    "correctPrediction=predictionFinal.filter(predictionFinal['prediction']==predictionFinal['Label']).count()\n",
    "totalData=predictionFinal.count()\n",
    "print(\"correct prediction:\",correctPrediction,\",total data:\",totalData,\",accuracy:\", correctPrediction/totalData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "appName=\"Clustering in Spark\"\n",
    "spark= SparkSession\\\n",
    "    .builder \\\n",
    "    .appName(appName) \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---+-------------+-----------+------+-------------+--------------+---------+----------+---------+----+\n",
      "|   CustomerName|Age|MaritalStatus|IncomeRange|Gender|TotalChildren|ChildrenAtHome|Education|Occupation|HomeOwner|Cars|\n",
      "+---------------+---+-------------+-----------+------+-------------+--------------+---------+----------+---------+----+\n",
      "|    Aaron Adams| 42|            0|      50000|     0|            0|             0|        3|         2|        1|   1|\n",
      "|Aaron Alexander| 40|            1|      50000|     0|            0|             0|        2|         2|        1|   2|\n",
      "|    Aaron Allen| 63|            0|      25000|     0|            2|             1|        2|         1|        1|   2|\n",
      "|    Aaron Baker| 56|            1|      50000|     0|            4|             2|        2|         2|        1|   2|\n",
      "|   Aaron Bryant| 72|            0|      75000|     0|            4|             0|        4|         4|        1|   2|\n",
      "+---------------+---+-------------+-----------+------+-------------+--------------+---------+----------+---------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customers =spark.read.csv('customers.csv',inferSchema=True,header=True)\n",
    "customers.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------------------------------------+\n",
      "|CustomerName   |features                                      |\n",
      "+---------------+----------------------------------------------+\n",
      "|Aaron Adams    |[42.0,0.0,50000.0,0.0,0.0,0.0,3.0,2.0,1.0,1.0]|\n",
      "|Aaron Alexander|[40.0,1.0,50000.0,0.0,0.0,0.0,2.0,2.0,1.0,2.0]|\n",
      "|Aaron Allen    |[63.0,0.0,25000.0,0.0,2.0,1.0,2.0,1.0,1.0,2.0]|\n",
      "+---------------+----------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembler=VectorAssembler(inputCols=[\"Age\",\"MaritalStatus\",\"IncomeRange\",\"Gender\",\"TotalChildren\",\"ChildrenAtHome\",\"Education\",\"Occupation\",\"HomeOwner\",\"Cars\"],outputCol=\"features\")\n",
    "data=assembler.transform(customers).select('CustomerName','features')\n",
    "data.show(truncate=False, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is successfully trained\n"
     ]
    }
   ],
   "source": [
    "kmeans=KMeans(featuresCol=assembler.getOutputCol(),predictionCol=\"cluster\",k=5)\n",
    "model=kmeans.fit(data)\n",
    "print(\"Model is successfully trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers: \n",
      "[5.60711289e+01 5.83804487e-01 7.50000000e+04 5.03921211e-01\n",
      " 2.17308043e+00 8.16706183e-01 3.73244574e+00 3.92759438e+00\n",
      " 7.23326646e-01 1.38063104e+00]\n",
      "[5.31013005e+01 4.17180014e-01 2.50000000e+04 4.80492813e-01\n",
      " 1.41512663e+00 6.08487337e-01 2.31622177e+00 1.45448323e+00\n",
      " 5.93086927e-01 1.11464750e+00]\n",
      "[5.53417813e+01 5.72411296e-01 1.00000000e+05 4.97103548e-01\n",
      " 2.54380883e+00 1.54272266e+00 3.46198407e+00 4.19116582e+00\n",
      " 7.16509776e-01 1.94532947e+00]\n",
      "[5.82794840e+01 6.22850123e-01 1.50000000e+05 4.79729730e-01\n",
      " 2.07248157e+00 3.20638821e+00 3.41461916e+00 4.34705160e+00\n",
      " 6.48648649e-01 3.10995086e+00]\n",
      "[5.19737441e+01 5.26868545e-01 5.00000000e+04 4.93961141e-01\n",
      " 1.34552774e+00 4.98337126e-01 3.23035183e+00 2.77927534e+00\n",
      " 6.62699107e-01 1.14615789e+00]\n"
     ]
    }
   ],
   "source": [
    "centers=model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|cluster|count|\n",
      "+-------+-----+\n",
      "|      0| 5483|\n",
      "|      1| 2922|\n",
      "|      2| 2762|\n",
      "|      3| 1628|\n",
      "|      4| 5713|\n",
      "+-------+-----+\n",
      "\n",
      "+---------------+-------+\n",
      "|   CustomerName|cluster|\n",
      "+---------------+-------+\n",
      "|    Aaron Adams|      4|\n",
      "|Aaron Alexander|      4|\n",
      "|    Aaron Allen|      1|\n",
      "|    Aaron Baker|      4|\n",
      "|   Aaron Bryant|      0|\n",
      "+---------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction=model.transform(data)\n",
    "prediction.groupBy(\"cluster\").count().orderBy(\"cluster\").show()\n",
    "prediction.select('CustomerName','cluster').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "appName= \"Recommender Syatem in Spark\"\n",
    "spark=SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(appName) \\\n",
    "    .config(\"saprk.some.config.option\",\"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+----------+--------------------+--------------------+\n",
      "|movieId|userId|rating| timestamp|               title|              genres|\n",
      "+-------+------+------+----------+--------------------+--------------------+\n",
      "|     31|     1|   2.5|1260759144|Dangerous Minds (...|               Drama|\n",
      "|   1029|     1|   3.0|1260759179|        Dumbo (1941)|Animation|Childre...|\n",
      "|   1061|     1|   3.0|1260759182|     Sleepers (1996)|            Thriller|\n",
      "|   1129|     1|   2.0|1260759185|Escape from New Y...|Action|Adventure|...|\n",
      "|   1172|     1|   4.0|1260759205|Cinema Paradiso (...|               Drama|\n",
      "|   1263|     1|   2.0|1260759151|Deer Hunter, The ...|           Drama|War|\n",
      "|   1287|     1|   2.0|1260759187|      Ben-Hur (1959)|Action|Adventure|...|\n",
      "|   1293|     1|   2.0|1260759148|       Gandhi (1982)|               Drama|\n",
      "|   1339|     1|   3.5|1260759125|Dracula (Bram Sto...|Fantasy|Horror|Ro...|\n",
      "|   1343|     1|   2.0|1260759131|    Cape Fear (1991)|            Thriller|\n",
      "+-------+------+------+----------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings=spark.read.csv('ratings.csv',inferSchema=True,header=True)\n",
    "movies =spark.read.csv('movies.csv',inferSchema=True,header=True)\n",
    "ratings.join(movies,\"movieId\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training data rows: 70015 ,number of testing data rows: 29989\n"
     ]
    }
   ],
   "source": [
    "data=ratings.select(\"userId\",\"movieId\",\"rating\")\n",
    "splits = data.randomSplit([0.7,0.3])\n",
    "train=splits[0].withColumnRenamed(\"rating\",\"label\")\n",
    "test=splits[1].withColumnRenamed(\"rating\",\"trueLabel\")\n",
    "train_rows=train.count()\n",
    "test_rows=test.count()\n",
    "print (\"number of training data rows:\", train_rows,\",number of testing data rows:\",test_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is Done\n"
     ]
    }
   ],
   "source": [
    "als=ALS(maxIter=19,regParam=0.01,userCol=\"userId\",itemCol=\"movieId\",ratingCol=\"label\")\n",
    "model=als.fit(train)\n",
    "print(\"Training is Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------------------+----------+---------+\n",
      "|userId|title                      |prediction|trueLabel|\n",
      "+------+---------------------------+----------+---------+\n",
      "|232   |Guilty as Sin (1993)       |4.400036  |4.0      |\n",
      "|242   |Guilty as Sin (1993)       |3.6995497 |4.0      |\n",
      "|311   |Guilty as Sin (1993)       |3.1570003 |3.0      |\n",
      "|126   |Hudsucker Proxy, The (1994)|4.0645285 |5.0      |\n",
      "|460   |Hudsucker Proxy, The (1994)|4.338686  |5.0      |\n",
      "|86    |Hudsucker Proxy, The (1994)|4.228522  |4.0      |\n",
      "|491   |Hudsucker Proxy, The (1994)|4.5939355 |3.0      |\n",
      "|92    |Hudsucker Proxy, The (1994)|4.9593315 |4.0      |\n",
      "|299   |Hudsucker Proxy, The (1994)|4.372009  |4.5      |\n",
      "|309   |Hudsucker Proxy, The (1994)|4.538096  |4.0      |\n",
      "+------+---------------------------+----------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction =model.transform(test)\n",
    "prediction.join(movies,\"movieId\").select(\"userId\",\"title\",\"prediction\",\"trueLabel\").show(n=10,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Square Error(RMSE): nan\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator=RegressionEvaluator(labelCol=\"trueLabel\",predictionCol=\"prediction\",metricName=\"rmse\")\n",
    "rmse=evaluator.evaluate(prediction)\n",
    "print(\"Root Mean Square Error(RMSE):\",rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of orignal data rows:  29989\n",
      "number of rows after droping missing data:  28733\n",
      "Number of Rows dropped:  1256\n"
     ]
    }
   ],
   "source": [
    "prediction.count()\n",
    "a=prediction.count()\n",
    "print(\"number of orignal data rows: \",a)\n",
    "cleanPred = prediction.dropna(how=\"any\", subset=['prediction'])\n",
    "b=cleanPred.count()\n",
    "print(\"number of rows after droping missing data: \",b)\n",
    "print(\"Number of Rows dropped: \",a-b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Square Error(RMSE): 1.2427419959597237\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator=RegressionEvaluator(labelCol=\"trueLabel\",predictionCol=\"prediction\",metricName=\"rmse\")\n",
    "rmse=evaluator.evaluate(cleanPred)\n",
    "print(\"Root Mean Square Error(RMSE):\",rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"StructuredNetworkWordCount\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()\n",
    "\n",
    "# Split the lines into words\n",
    "words = lines.select(\n",
    "   explode(\n",
    "       split(lines.value, \" \")\n",
    "   ).alias(\"word\")\n",
    ")\n",
    "\n",
    "# Generate running word count\n",
    "wordCounts = words.groupBy(\"word\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating file log1.txt\n",
      "creating file log2.txt\n",
      "creating file log3.txt\n",
      "creating file log4.txt\n",
      "creating file log5.txt\n",
      "creating file log6.txt\n",
      "creating file log7.txt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-5987d84a8a3d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-18-5987d84a8a3d>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'creating file log{}.txt'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0ma\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "import time\n",
    "\n",
    "\"\"\"\n",
    "This is use for create 30 file one by one in each 5 seconds interval. \n",
    "These files will store content dynamically from 'lorem.txt' using below code\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def main():\n",
    "    a = 1\n",
    "    with open('lorem.txt', 'r') as file:  # reading content from 'lorem.txt' file\n",
    "        lines = file.readlines()\n",
    "        while a <= 30:\n",
    "            totalline = len(lines)\n",
    "            linenumber = randint(0, totalline - 10)\n",
    "            with open('log\\log{}.txt'.format(a), 'w') as writefile:\n",
    "                writefile.write(' '.join(line for line in lines[linenumber:totalline]))\n",
    "            print('creating file log{}.txt'.format(a))\n",
    "            a += 1\n",
    "            time.sleep(5)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2019-02-07 16:28:09\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-02-07 16:28:12\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-02-07 16:28:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-02-07 16:28:18\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-02-07 16:28:21\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "\"\"\"\n",
    "This is use for create streaming of text from txt files that creating dynamically \n",
    "from files.py code. This spark streaming will execute in each 3 seconds and It'll\n",
    "show number of words count from each files dynamically\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def main():\n",
    "    ssc = StreamingContext(sc, 3)   #Streaming will execute in each 3 seconds\n",
    "    lines = ssc.textFileStream('log/')  #'log/ mean directory name\n",
    "    counts = lines.flatMap(lambda line: line.split(\" \")) \\\n",
    "        .map(lambda x: (x, 1)) \\\n",
    "        .reduceByKey(lambda a, b: a + b)\n",
    "    counts.pprint()\n",
    "    ssc.start()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "sc.stop()\n",
    "# Create a local StreamingContext with two working thread and batch interval of 1 second\n",
    "sc = SparkContext(\"local[2]\", \"NetworkWordCount\")\n",
    "ssc = StreamingContext(sc, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = ssc.socketTextStream(\"localhost\", 9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = lines.flatMap(lambda line: line.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = words.map(lambda word: (word, 1))\n",
    "wordCounts = pairs.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Print the first ten elements of each RDD generated in this DStream to the console\n",
    "wordCounts.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2019-02-07 16:25:40\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-02-07 16:25:41\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-02-07 16:25:42\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-02-07 16:25:43\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-02-07 16:25:44\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-02-07 16:25:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-02-07 16:25:46\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-02-07 16:25:47\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-02-07 16:25:48\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-02-07 16:25:49\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-02-07 16:25:50\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-02-07 16:25:51\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-02-07 16:25:52\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-02-07 16:25:53\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-02-07 16:25:54\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-02-07 16:25:55\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-02-07 16:25:56\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-02-07 16:25:57\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-02-07 16:25:58\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-02-07 16:25:59\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-02-07 16:26:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-02-07 16:26:01\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-02-07 16:26:02\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-02-07 16:26:03\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-02-07 16:26:04\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-02-07 16:26:05\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-02-07 16:26:06\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-02-07 16:26:07\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-02-07 16:26:08\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-02-07 16:26:09\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-02-07 16:26:10\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-02-07 16:26:11\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-02-07 16:26:12\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-02-07 16:26:13\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc.start()             # Start the computation\n",
    "ssc.awaitTermination()  # Wait for the computation to terminate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
